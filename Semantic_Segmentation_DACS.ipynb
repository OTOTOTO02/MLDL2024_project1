{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWIvDpquM-Tl"
      },
      "source": [
        "# Preparations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ia5-WeZ0op3"
      },
      "source": [
        "## Variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5n1Dm1A0gjl"
      },
      "source": [
        "### Logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_HX9maPV0iVa"
      },
      "outputs": [],
      "source": [
        "ENABLE_PRINT = False\n",
        "ENABLE_WANDB_LOG = True\n",
        "log_per_epoch = 20\n",
        "n_classes = 19\n",
        "\n",
        "train_step = 0\n",
        "val_step = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jntcuf0zNwaq"
      },
      "source": [
        "### Device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ie8Own_ANu_t",
        "outputId": "ff9ae26c-5dcf-4a30-d21a-f579ab543a84"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "# device = torch.device('cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPZY7dOBM6_m"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9TMCaEmg1N4"
      },
      "source": [
        "### Label-color correlator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZjUPxdZcbc-N"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import Tuple\n",
        "\n",
        "@dataclass\n",
        "class GTA5Label:\n",
        "    name: str\n",
        "    ID: int\n",
        "    color: Tuple[int, int, int]\n",
        "\n",
        "class GTA5Labels_TaskCV2017():\n",
        "    road = GTA5Label(name = \"road\", ID=0, color=(128, 64, 128))\n",
        "    sidewalk = GTA5Label(name = \"sidewalk\", ID=1, color=(244, 35, 232))\n",
        "    building = GTA5Label(name = \"building\", ID=2, color=(70, 70, 70))\n",
        "    wall = GTA5Label(name = \"wall\", ID=3, color=(102, 102, 156))\n",
        "    fence = GTA5Label(name = \"fence\", ID=4, color=(190, 153, 153))\n",
        "    pole = GTA5Label(name = \"pole\", ID=5, color=(153, 153, 153))\n",
        "    light = GTA5Label(name = \"light\", ID=6, color=(250, 170, 30))\n",
        "    sign = GTA5Label(name = \"sign\", ID=7, color=(220, 220, 0))\n",
        "    vegetation = GTA5Label(name = \"vegetation\", ID=8, color=(107, 142, 35))\n",
        "    terrain = GTA5Label(name = \"terrain\", ID=9, color=(152, 251, 152))\n",
        "    sky = GTA5Label(name = \"sky\", ID=10, color=(70, 130, 180))\n",
        "    person = GTA5Label(name = \"person\", ID=11, color=(220, 20, 60))\n",
        "    rider = GTA5Label(name = \"rider\", ID=12, color=(255, 0, 0))\n",
        "    car = GTA5Label(name = \"car\", ID=13, color=(0, 0, 142))\n",
        "    truck = GTA5Label(name = \"truck\", ID=14, color=(0, 0, 70))\n",
        "    bus = GTA5Label(name = \"bus\", ID=15, color=(0, 60, 100))\n",
        "    train = GTA5Label(name = \"train\", ID=16, color=(0, 80, 100))\n",
        "    motocycle = GTA5Label(name = \"motocycle\", ID=17, color=(0, 0, 230))\n",
        "    bicycle = GTA5Label(name = \"bicycle\", ID=18, color=(119, 11, 32))\n",
        "    void = GTA5Label(name = \"void\", ID=255, color=(0,0,0))\n",
        "\n",
        "    list_ = [\n",
        "        road,\n",
        "        sidewalk,\n",
        "        building,\n",
        "        wall,\n",
        "        fence,\n",
        "        pole,\n",
        "        light,\n",
        "        sign,\n",
        "        vegetation,\n",
        "        terrain,\n",
        "        sky,\n",
        "        person,\n",
        "        rider,\n",
        "        car,\n",
        "        truck,\n",
        "        bus,\n",
        "        train,\n",
        "        motocycle,\n",
        "        bicycle,\n",
        "        void\n",
        "    ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxswDJZOmLqQ"
      },
      "source": [
        "### Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fjLlz9y2jfn",
        "outputId": "5231a0dd-2146-4963-840f-68802a4c5457"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import wandb\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import zipfile\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import time\n",
        "\n",
        "# !pip -q install -U fvcore\n",
        "\n",
        "# from fvcore.nn import FlopCountAnalysis\n",
        "\n",
        "def pretty_extract(zip_path:str, extract_to:str) -> None:\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        png_files = [f for f in zip_ref.namelist() if f.endswith('.png')]\n",
        "\n",
        "        for file in tqdm(png_files, desc=\"Extracting PNGs\"):\n",
        "            zip_ref.extract(file, path=extract_to)\n",
        "\n",
        "def poly_lr_scheduler(optimizer, init_lr:float, iter:int=0, lr_decay_iter:int=1, max_iter:int=50, power:float=0.9) -> float:\n",
        "    \"\"\"Polynomial decay of learning rate\n",
        "            :param init_lr is base learning rate\n",
        "            :param iter is a current iteration\n",
        "            :param lr_decay_iter how frequently decay occurs, default is 1\n",
        "            :param max_iter is number of maximum iterations\n",
        "            :param power is a polymomial power\n",
        "\n",
        "    \"\"\"\n",
        "    if ((iter % lr_decay_iter) != 0) or iter > max_iter:\n",
        "        return optimizer.param_groups[0]['lr']\n",
        "\n",
        "    lr = init_lr*(1 - iter/max_iter)**power\n",
        "    optimizer.param_groups[0]['lr'] = lr\n",
        "    return lr\n",
        "\n",
        "def fast_hist(a:np.ndarray, b:np.ndarray, n:int) -> np.ndarray:\n",
        "    '''\n",
        "    a and b are label and prediction respectively\n",
        "    n is the number of classes\n",
        "    '''\n",
        "    k = (a >= 0) & (a < n)\n",
        "    return np.bincount(n * a[k].astype(int) + b[k], minlength=n ** 2).reshape(n, n)\n",
        "\n",
        "def fast_hist_torch_cuda(a: torch.Tensor, b: torch.Tensor, n: int) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    a and b are label and prediction respectively.\n",
        "    n is the number of classes.\n",
        "    This version works with CUDA tensors.\n",
        "    \"\"\"\n",
        "    k = (a >= 0) & (a < n)\n",
        "    a = a[k].to(torch.int64)\n",
        "    b = b[k].to(torch.int64)\n",
        "\n",
        "    indices = n * a + b\n",
        "\n",
        "    hist = torch.zeros(n * n, dtype=torch.int64, device=a.device)\n",
        "\n",
        "    hist.scatter_add_(0, indices, torch.ones_like(indices, dtype=torch.int64))\n",
        "\n",
        "    return hist.view(n, n)\n",
        "\n",
        "def per_class_iou(hist:np.ndarray) -> np.ndarray:\n",
        "    epsilon = 1e-5\n",
        "    return (np.diag(hist)) / (hist.sum(1) + hist.sum(0) - np.diag(hist) + epsilon)\n",
        "\n",
        "def per_class_iou_cuda(hist:torch.Tensor) -> torch.Tensor:\n",
        "    epsilon = 1e-5\n",
        "    diag = torch.diag(hist)\n",
        "\n",
        "    sum_rows = hist.sum(dim=1)\n",
        "    sum_cols = hist.sum(dim=0)\n",
        "\n",
        "    iou = diag / (sum_rows + sum_cols - diag + epsilon)\n",
        "    return iou\n",
        "\n",
        "# Mapping labelId image to RGB image\n",
        "def decode_segmap(mask:np.ndarray) -> np.ndarray:\n",
        "    h, w = mask.shape\n",
        "    color_mask = np.zeros((h, w, 3), dtype=np.uint8)\n",
        "    for label_id in GTA5Labels_TaskCV2017().list_:\n",
        "        color_mask[mask == label_id.ID, :] = label_id.color\n",
        "\n",
        "    return color_mask\n",
        "\n",
        "def tensorToImageCompatible(t:torch.Tensor) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    convert from a tensor of shape [C, H, W] where a normalization has been applied\n",
        "    to an unnormalized tensor of shape [H, W, C],\n",
        "    so *plt.imshow(tensorToImageCompatible(tensor))* works as expected.\\n\n",
        "    Intended to be used to recover the original element\n",
        "    when this transformation is used:\n",
        "    - transform = TF.Compose([\n",
        "        TF.ToTensor(),\n",
        "        TF.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                    std=[0.229, 0.224, 0.225])])\n",
        "    \"\"\"\n",
        "    mean = torch.tensor([0.485, 0.456, 0.406]).view([-1, 1, 1])\n",
        "    std = torch.tensor([0.229, 0.224, 0.225]).view([-1, 1, 1])\n",
        "\n",
        "    unnormalized = t * std + mean\n",
        "\n",
        "    return (unnormalized.permute(1,2,0).clamp(0,1).numpy()*255).astype(np.uint8)\n",
        "\n",
        "def log_confusion_matrix(title:str, hist:np.ndarray, tag:str, step_name:str, step_value:int):\n",
        "    row_sums = hist.sum(axis=1, keepdims=True)\n",
        "    safe_hist = np.where(row_sums == 0, 0, hist / row_sums)\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(100.*safe_hist, fmt=\".2f\", annot=True, cmap=\"Blues\", annot_kws={'size': 7})\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"True\")\n",
        "    plt.title(title)\n",
        "\n",
        "    wandb.log({tag: wandb.Image(plt), step_name:step_value})\n",
        "    plt.close()\n",
        "\n",
        "def log_bar_chart_ioU(title:str, class_names:list, mIou:float, iou_class:np.ndarray, tag:str, step_name:str, epoch:int):\n",
        "    iou_percent = [round(iou*100., 2) for iou in iou_class]\n",
        "    miou_percent = round(mIou*100., 2)\n",
        "\n",
        "    all_labels = [\"mIoU\"] + class_names\n",
        "    all_values = [miou_percent] + iou_percent\n",
        "\n",
        "    plt.figure(figsize=(14, 5))\n",
        "    bars = plt.bar(range(len(all_values)), all_values, color='skyblue')\n",
        "    plt.xticks(range(len(all_labels)), all_labels, rotation=45, ha=\"right\")\n",
        "\n",
        "    for i, bar in enumerate(bars):\n",
        "        height = bar.get_height()\n",
        "        plt.text(bar.get_x() + bar.get_width() / 2.0, height + 1, f'{height:.2f}',\n",
        "                ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "    plt.ylabel(\"IoU (%)\")\n",
        "    plt.ylim(0, 105)\n",
        "    plt.title(title)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    wandb.log({tag: wandb.Image(plt), step_name:epoch})\n",
        "\n",
        "    plt.close()\n",
        "\n",
        "def num_flops(device, model:torch.nn.Module, H:int, W:int):\n",
        "    model.eval()\n",
        "    img = (torch.zeros((1,3,H,W), device=device),)\n",
        "\n",
        "    flops = FlopCountAnalysis(model, img)\n",
        "    return flops.total()/1e9\n",
        "\n",
        "def num_param(model: torch.nn.Module):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)/1e6\n",
        "\n",
        "def latency(device, model:torch.nn.Module, H:int, W:int):\n",
        "    model.eval()\n",
        "\n",
        "    img = torch.zeros((1,3,H,W)).to(device)\n",
        "    iterations = 100\n",
        "    latency_list = []\n",
        "    FPS_list  = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in tqdm(range(iterations)):\n",
        "            start_time = time.time()\n",
        "            _ = model(img)\n",
        "            end_time = time.time()\n",
        "\n",
        "            latency = end_time - start_time\n",
        "\n",
        "            latency_list.append(latency)\n",
        "            FPS_list.append(1.0/latency)\n",
        "\n",
        "    mean_latency = np.mean(latency_list)*1000\n",
        "    std_latency = np.std(latency_list)*1000\n",
        "    mean_FPS = np.mean(FPS_list)\n",
        "\n",
        "    return mean_latency, std_latency, mean_FPS\n",
        "\n",
        "class ClassMixer():\n",
        "    def __init__(self, n_classes:int):\n",
        "        self.n_classes = n_classes\n",
        "        self.mask = None\n",
        "        self.mask_img = None\n",
        "\n",
        "    def create_mask(self, source_labels):\n",
        "        B = source_labels.size(0)\n",
        "        \n",
        "        self.mask = torch.zeros_like(source_labels, dtype=torch.bool, requires_grad=False)\n",
        "\n",
        "        for i in range(B):\n",
        "            selected_classes = torch.randperm(n_classes)[:n_classes//2]\n",
        "\n",
        "            self.mask[i] |= torch.isin(source_labels[i], selected_classes)\n",
        "        \n",
        "    def mix_img(self, source_imgs, target_imgs):\n",
        "        self.mask_img = self.mask.unsqueeze(1).expand_as(source_imgs)\n",
        "        mixed_img = torch.where(self.mask_img, source_imgs, target_imgs)\n",
        "        return mixed_img\n",
        "    \n",
        "    def mix_label(self, source_labels, target_pseudo_labels):\n",
        "        mixed_label = torch.where(self.mask, source_labels, target_pseudo_labels)\n",
        "        return mixed_label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_classes = 19\n",
        "model = BiSeNet(n_classes, \"resnet18\").to(\"cpu\")\n",
        "model.eval()\n",
        "\n",
        "B, C, H, W = 4,3,32,32\n",
        "source_img = torch.randint(0,100, (B, C, H, W), dtype=torch.float32)\n",
        "source_label = torch.randint(0,3, (B, H, W), dtype=torch.uint8)\n",
        "target_img = torch.randint(200, 256, (B//2, C, H, W), dtype=torch.float32)\n",
        "target_img[0,:, H//2-3:H//2+3, W//2-3:W//2+3] = torch.randint(50,100, (C,6,6))\n",
        "\n",
        "classmixer = ClassMixer(n_classes)\n",
        "\n",
        "target_pseudo_label = model(target_img).argmax(1)\n",
        "\n",
        "classmixer.create_mask(source_label[B//2:])\n",
        "\n",
        "m_img, m_label = classmixer.mix(source_img[B//2:], source_label[B//2:], target_img, target_pseudo_label)\n",
        "\n",
        "for i in range(0,B//2):\n",
        "    fig = plt.figure(figsize=(10, 5))\n",
        "    axs = fig.subplots(2,3)\n",
        "\n",
        "    axs[0,0].imshow(\n",
        "        source_img[i+B//2].permute(1,2,0)/255.0\n",
        "    )\n",
        "    axs[0,1].imshow(\n",
        "        m_img[i].permute(1,2,0)/255.0\n",
        "    )\n",
        "    axs[0,2].imshow(\n",
        "        target_img[i].permute(1,2,0)/255.0\n",
        "    )\n",
        "    axs[1,0].imshow(decode_segmap(source_label[i+B//2]))\n",
        "    axs[1,1].imshow(decode_segmap(m_label[i]))\n",
        "    axs[1,2].imshow(decode_segmap(target_pseudo_label[i]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Downloads"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLfNdNzyMyxn"
      },
      "source": [
        "## CityScapes download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154,
          "referenced_widgets": [
            "249fb549c78d48c58ad588a3596f3825",
            "0e449ad830c34088b31d0cf60c6cf224",
            "de610e2099e245dd80b6fbd6b39da0fe",
            "07533cefef9147be90b61bff6e94280b",
            "6aa9795250e74752acc7a342762a0972",
            "589369f18fe24a8b994f1a53fdeda49a",
            "ea2cfa72ef024c55b0feb63bf5d40e31",
            "052c813485864162906ce72faede3370",
            "ae13707e46444778addfbb572b2efbe7",
            "c16b236d7dc74c999529c30b6abdf89d",
            "bf7d292d791e4d3ea70e445856e35744"
          ]
        },
        "id": "B5p-mIhc2uIx",
        "outputId": "a8d68244-7460-42f6-ec02-ad006a56f2d1"
      },
      "outputs": [],
      "source": [
        "# !pip install -q gdown\n",
        "\n",
        "# file_id = \"1IyAqnm_NLDR7rvMNTBe0QrNEv7HHHW4p\"\n",
        "# !gdown https://drive.google.com/uc?id={file_id}\n",
        "\n",
        "# pretty_extract(\"Cityscapes.zip\", \".\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZME_xiNNE-g"
      },
      "source": [
        "## Cityscapes implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRCoySxr2fnu"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "from PIL import Image\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class CityScapes(Dataset):\n",
        "    def __init__(self, rootdir, split=\"train\", imgdir=\"images\", labeldir=\"gtFine\", transform=None, label_transform=None):\n",
        "        super(CityScapes, self).__init__()\n",
        "\n",
        "        self.rootdir = rootdir\n",
        "        self.split = split\n",
        "        self.labeldir = os.path.join(self.rootdir, labeldir, self.split) # ./gtFine/train/\n",
        "        self.imgdir = os.path.join(self.rootdir, imgdir, self.split) # ./images/train/\n",
        "        self.transform = transform\n",
        "        self.label_transform = label_transform\n",
        "\n",
        "        self.imgs_path = []\n",
        "        self.labels_color_path = []\n",
        "        self.labels_labelIds_path = []\n",
        "\n",
        "        for city in os.listdir(self.imgdir): # frankfurt\n",
        "            img_city_dir = os.path.join(self.imgdir, city) # ./images/train/frankfurt/\n",
        "            label_city_dir = os.path.join(self.labeldir, city) # ./gtFine/train/frankfurt/\n",
        "\n",
        "            for img_path in os.listdir(img_city_dir): # frankfurt_000000_000294_leftImg8bit.png\n",
        "                if img_path.endswith(\".png\"):\n",
        "                  self.imgs_path.append(os.path.join(img_city_dir, img_path)) # ./images/train/frankfurt/frankfurt_000000_000294_leftImg8bit.png\n",
        "\n",
        "                  label_color_path = img_path.replace(\"leftImg8bit\", \"gtFine_color\") # frankfurt_000000_000294_gtFine_color.png\n",
        "                  label_labelIds_path = img_path.replace(\"leftImg8bit\", \"gtFine_labelTrainIds\") # frankfurt_000000_000294_gtFine_labelTrainIds.png\n",
        "\n",
        "                  self.labels_color_path.append(os.path.join(label_city_dir, label_color_path)) # ./gtFine/train/frankfurt/frankfurt_000000_000294_gtFine_color.png\n",
        "                  self.labels_labelIds_path.append(os.path.join(label_city_dir, label_labelIds_path)) # ./gtFine/train/frankfurt/frankfurt_000000_000294_gtFine_labelTrainIds.png\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.imgs_path[idx]).convert('RGB')\n",
        "        label_color = Image.open(self.labels_color_path[idx]).convert('RGB')\n",
        "        label_labelIds = cv2.imread(self.labels_labelIds_path[idx], cv2.IMREAD_UNCHANGED).astype(np.long)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "            # label_color = self.transform(label_color)\n",
        "        if self.label_transform is not None:\n",
        "            label_labelIds = self.label_transform(label_labelIds)\n",
        "\n",
        "        return image, label_color, label_labelIds\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTki3m7aNH10"
      },
      "source": [
        "## GTA5 implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9PeGPXYNKvT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "from PIL import Image\n",
        "\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "import torchvision.transforms as TF\n",
        "from torchvision.transforms import v2\n",
        "\n",
        "# TODO: decide other augmentations\n",
        "class GTA5(Dataset):\n",
        "    def __init__(self, rootdir, file_names, imgdir=\"images\", targetdir=\"labels\", augment=False, transform=None, target_transform=None):\n",
        "        super(GTA5, self).__init__()\n",
        "\n",
        "        self.to_tensor = TF.ToTensor()\n",
        "        self.to_pil = TF.ToPILImage()\n",
        "        self.gaussian_blur = v2.GaussianBlur(5) #9, 2\n",
        "        self.color_jitter = TF.ColorJitter(brightness=0.1, contrast=0.2, saturation=0.5, hue=0.1) # 0.6, 0.6, 0.6, 0.1\n",
        "        self.gaussian_noise = v2.GaussianNoise(mean=0.0, sigma=0.1)\n",
        "\n",
        "        self.rootdir = rootdir\n",
        "\n",
        "        self.targetdir = os.path.join(self.rootdir, targetdir) # ./labels\n",
        "        self.imgdir = os.path.join(self.rootdir, imgdir) # ./images\n",
        "\n",
        "        self.augment = augment\n",
        "\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "        self.imgs_path = []\n",
        "        self.targets_color_path = []\n",
        "        self.targets_labelIds_path = []\n",
        "\n",
        "        for image_file in file_names: # 00001.png\n",
        "            self.imgs_path.append(os.path.join(self.imgdir, image_file)) #./images/00001.png\n",
        "\n",
        "            target_color_file = image_file # 00001.png\n",
        "            target_labelsId_file = image_file.split(\".\")[0]+\"_labelIds.png\" # 00001_labelIds.png\n",
        "\n",
        "            self.targets_color_path.append(os.path.join(self.targetdir, target_color_file)) #./labels/00001.png\n",
        "            self.targets_labelIds_path.append(os.path.join(self.targetdir, target_labelsId_file)) #./labels/00001_labelIDs.png\n",
        "\n",
        "    def create_target_img(self):\n",
        "        list_ = GTA5Labels_TaskCV2017().list_\n",
        "\n",
        "        for i, img_path in tqdm(enumerate(self.targets_color_path)):\n",
        "            image_numpy = np.asarray(Image.open(img_path).convert('RGB'))\n",
        "\n",
        "            H, W, _ = image_numpy.shape\n",
        "            label_image = 255*np.ones((H, W), dtype=np.uint8)\n",
        "\n",
        "            for label in list_:\n",
        "                label_image[(image_numpy == label.color).all(axis=-1)] = label.ID\n",
        "\n",
        "            new_img = Image.fromarray(label_image)\n",
        "            new_img.save(self.targets_labelIds_path[i])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.imgs_path[idx]).convert('RGB')\n",
        "\n",
        "        target_color = Image.open(self.targets_color_path[idx]).convert('RGB')\n",
        "        target_labelIds = cv2.imread(self.targets_labelIds_path[idx], cv2.IMREAD_UNCHANGED).astype(np.int64)\n",
        "\n",
        "        if self.augment:\n",
        "            image, target_color, target_labelIds = self.augment_data(image, target_color, target_labelIds)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "            # target_color = self.transform(target_color)\n",
        "        if self.target_transform is not None:\n",
        "            target_labelIds = self.target_transform(target_labelIds)\n",
        "\n",
        "        return image, target_color, target_labelIds\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs_path)\n",
        "\n",
        "    def augment_data(self, image, target_color, target_labelIds):\n",
        "        # original = image.copy()\n",
        "        val = random.random()\n",
        "\n",
        "        if val < 0.5:\n",
        "            img_tensor = self.to_tensor(image)\n",
        "\n",
        "        # Geometric Transformations\n",
        "            # Horizontal Flip\n",
        "            # img_tensor = TF.functional.hflip(img_tensor)\n",
        "            # target_color = np.fliplr(target_color).copy()\n",
        "            # target_labelIds = np.fliplr(target_labelIds).copy()\n",
        "\n",
        "        # Photometric Transformations\n",
        "            # Gaussian Blur\n",
        "            image = self.gaussian_blur(img_tensor)\n",
        "\n",
        "            # Color Jitter\n",
        "            img_tensor = self.color_jitter(img_tensor)\n",
        "\n",
        "            # Gaussian Noise\n",
        "            img_tensor = self.gaussian_noise(img_tensor)\n",
        "\n",
        "            img_tensor = torch.clamp(img_tensor, 0.0, 1.0)\n",
        "            image = self.to_pil(img_tensor)\n",
        "\n",
        "        return image, target_color, target_labelIds\n",
        "\n",
        "\n",
        "def GTA5_dataset_splitter(rootdir, train_split_percent, split_seed = None, imgdir=\"images\", targetdir=\"labels\", augment=False, transform=None, target_transform=None):\n",
        "    assert 0.0 <= train_split_percent <= 1.0, \"train_split_percent should be a float between 0 and 1\"\n",
        "\n",
        "    target_path = os.path.join(rootdir, targetdir) # ./labels\n",
        "    img_path = os.path.join(rootdir, imgdir) # ./images\n",
        "\n",
        "    file_names = [\n",
        "        f for f in os.listdir(img_path)\n",
        "        if f.endswith(\".png\") and os.path.exists(os.path.join(target_path, f.split(\".\")[0]+\"_labelIds.png\"))\n",
        "    ]\n",
        "\n",
        "    if split_seed is not None:\n",
        "        random.seed(split_seed)\n",
        "    random.shuffle(file_names)\n",
        "    random.seed()\n",
        "\n",
        "    split_idx = int(len(file_names) * train_split_percent)\n",
        "\n",
        "    train_files = file_names[:split_idx]\n",
        "    val_files = file_names[split_idx:]\n",
        "\n",
        "    return GTA5(rootdir, train_files, imgdir, targetdir, augment, transform, target_transform), \\\n",
        "           GTA5(rootdir, val_files, imgdir, targetdir, False, transform, target_transform)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2ybDTYzv0ZS"
      },
      "source": [
        "# Bisenet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MbAqrak4u5Ce"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')\n",
        "\n",
        "import torch\n",
        "from torchvision import models\n",
        "\n",
        "class resnet18(torch.nn.Module):\n",
        "    def __init__(self, pretrained=True):\n",
        "        super().__init__()\n",
        "        self.features = models.resnet18(pretrained=pretrained)\n",
        "        self.conv1 = self.features.conv1\n",
        "        self.bn1 = self.features.bn1\n",
        "        self.relu = self.features.relu\n",
        "        self.maxpool1 = self.features.maxpool\n",
        "        self.layer1 = self.features.layer1\n",
        "        self.layer2 = self.features.layer2\n",
        "        self.layer3 = self.features.layer3\n",
        "        self.layer4 = self.features.layer4\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = self.conv1(input)\n",
        "        x = self.relu(self.bn1(x))\n",
        "        x = self.maxpool1(x)\n",
        "        feature1 = self.layer1(x)  # 1 / 4\n",
        "        feature2 = self.layer2(feature1)  # 1 / 8\n",
        "        feature3 = self.layer3(feature2)  # 1 / 16\n",
        "        feature4 = self.layer4(feature3)  # 1 / 32\n",
        "        # global average pooling to build tail\n",
        "        tail = torch.mean(feature4, 3, keepdim=True)\n",
        "        tail = torch.mean(tail, 2, keepdim=True)\n",
        "        return feature3, feature4, tail\n",
        "\n",
        "\n",
        "class resnet101(torch.nn.Module):\n",
        "    def __init__(self, pretrained=True):\n",
        "        super().__init__()\n",
        "        self.features = models.resnet101(pretrained=pretrained)\n",
        "        self.conv1 = self.features.conv1\n",
        "        self.bn1 = self.features.bn1\n",
        "        self.relu = self.features.relu\n",
        "        self.maxpool1 = self.features.maxpool\n",
        "        self.layer1 = self.features.layer1\n",
        "        self.layer2 = self.features.layer2\n",
        "        self.layer3 = self.features.layer3\n",
        "        self.layer4 = self.features.layer4\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = self.conv1(input)\n",
        "        x = self.relu(self.bn1(x))\n",
        "        x = self.maxpool1(x)\n",
        "        feature1 = self.layer1(x)  # 1 / 4\n",
        "        feature2 = self.layer2(feature1)  # 1 / 8\n",
        "        feature3 = self.layer3(feature2)  # 1 / 16\n",
        "        feature4 = self.layer4(feature3)  # 1 / 32\n",
        "        # global average pooling to build tail\n",
        "        tail = torch.mean(feature4, 3, keepdim=True)\n",
        "        tail = torch.mean(tail, 2, keepdim=True)\n",
        "        return feature3, feature4, tail\n",
        "\n",
        "\n",
        "def build_contextpath(name):\n",
        "    model = {\n",
        "        'resnet18': resnet18(pretrained=True),\n",
        "        'resnet101': resnet101(pretrained=True)\n",
        "    }\n",
        "    return model[name]\n",
        "\n",
        "class ConvBlock(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=2, padding=1):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size,\n",
        "                               stride=stride, padding=padding, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = self.conv1(input)\n",
        "        return self.relu(self.bn(x))\n",
        "\n",
        "\n",
        "class Spatial_path(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.convblock1 = ConvBlock(in_channels=3, out_channels=64)\n",
        "        self.convblock2 = ConvBlock(in_channels=64, out_channels=128)\n",
        "        self.convblock3 = ConvBlock(in_channels=128, out_channels=256)\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = self.convblock1(input)\n",
        "        x = self.convblock2(x)\n",
        "        x = self.convblock3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class AttentionRefinementModule(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.in_channels = in_channels\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
        "\n",
        "    def forward(self, input):\n",
        "        # global average pooling\n",
        "        x = self.avgpool(input)\n",
        "        assert self.in_channels == x.size(1), 'in_channels and out_channels should all be {}'.format(x.size(1))\n",
        "        x = self.conv(x)\n",
        "        x = self.sigmoid(self.bn(x))\n",
        "        # x = self.sigmoid(x)\n",
        "        # channels of input and x should be same\n",
        "        x = torch.mul(input, x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class FeatureFusionModule(torch.nn.Module):\n",
        "    def __init__(self, num_classes, in_channels):\n",
        "        super().__init__()\n",
        "        # self.in_channels = input_1.channels + input_2.channels\n",
        "        # resnet101 3328 = 256(from spatial path) + 1024(from context path) + 2048(from context path)\n",
        "        # resnet18  1024 = 256(from spatial path) + 256(from context path) + 512(from context path)\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        self.convblock = ConvBlock(in_channels=self.in_channels, out_channels=num_classes, stride=1)\n",
        "        self.conv1 = nn.Conv2d(num_classes, num_classes, kernel_size=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.conv2 = nn.Conv2d(num_classes, num_classes, kernel_size=1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
        "\n",
        "    def forward(self, input_1, input_2):\n",
        "        x = torch.cat((input_1, input_2), dim=1)\n",
        "        assert self.in_channels == x.size(1), 'in_channels of ConvBlock should be {}'.format(x.size(1))\n",
        "        feature = self.convblock(x)\n",
        "        x = self.avgpool(feature)\n",
        "\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.sigmoid(self.conv2(x))\n",
        "        x = torch.mul(feature, x)\n",
        "        x = torch.add(x, feature)\n",
        "        return x\n",
        "\n",
        "\n",
        "class BiSeNet(torch.nn.Module):\n",
        "    def __init__(self, num_classes, context_path):\n",
        "        super().__init__()\n",
        "        # build spatial path\n",
        "        self.saptial_path = Spatial_path()\n",
        "\n",
        "        # build context path\n",
        "        self.context_path = build_contextpath(name=context_path)\n",
        "\n",
        "        # build attention refinement module  for resnet 101\n",
        "        if context_path == 'resnet101':\n",
        "            self.attention_refinement_module1 = AttentionRefinementModule(1024, 1024)\n",
        "            self.attention_refinement_module2 = AttentionRefinementModule(2048, 2048)\n",
        "            # supervision block\n",
        "            self.supervision1 = nn.Conv2d(in_channels=1024, out_channels=num_classes, kernel_size=1)\n",
        "            self.supervision2 = nn.Conv2d(in_channels=2048, out_channels=num_classes, kernel_size=1)\n",
        "            # build feature fusion module\n",
        "            self.feature_fusion_module = FeatureFusionModule(num_classes, 3328)\n",
        "\n",
        "        elif context_path == 'resnet18':\n",
        "            # build attention refinement module  for resnet 18\n",
        "            self.attention_refinement_module1 = AttentionRefinementModule(256, 256)\n",
        "            self.attention_refinement_module2 = AttentionRefinementModule(512, 512)\n",
        "            # supervision block\n",
        "            self.supervision1 = nn.Conv2d(in_channels=256, out_channels=num_classes, kernel_size=1)\n",
        "            self.supervision2 = nn.Conv2d(in_channels=512, out_channels=num_classes, kernel_size=1)\n",
        "            # build feature fusion module\n",
        "            self.feature_fusion_module = FeatureFusionModule(num_classes, 1024)\n",
        "        else:\n",
        "            print('Error: unspport context_path network \\n')\n",
        "\n",
        "        # build final convolution\n",
        "        self.conv = nn.Conv2d(in_channels=num_classes, out_channels=num_classes, kernel_size=1)\n",
        "\n",
        "        self.init_weight()\n",
        "\n",
        "        self.mul_lr = []\n",
        "        self.mul_lr.append(self.saptial_path)\n",
        "        self.mul_lr.append(self.attention_refinement_module1)\n",
        "        self.mul_lr.append(self.attention_refinement_module2)\n",
        "        self.mul_lr.append(self.supervision1)\n",
        "        self.mul_lr.append(self.supervision2)\n",
        "        self.mul_lr.append(self.feature_fusion_module)\n",
        "        self.mul_lr.append(self.conv)\n",
        "\n",
        "    def init_weight(self):\n",
        "        for name, m in self.named_modules():\n",
        "            if 'context_path' not in name:\n",
        "                if isinstance(m, nn.Conv2d):\n",
        "                    nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
        "                elif isinstance(m, nn.BatchNorm2d):\n",
        "                    m.eps = 1e-5\n",
        "                    m.momentum = 0.1\n",
        "                    nn.init.constant_(m.weight, 1)\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, input):\n",
        "        # output of spatial path\n",
        "        sx = self.saptial_path(input)\n",
        "\n",
        "        # output of context path\n",
        "        cx1, cx2, tail = self.context_path(input)\n",
        "        cx1 = self.attention_refinement_module1(cx1)\n",
        "        cx2 = self.attention_refinement_module2(cx2)\n",
        "        cx2 = torch.mul(cx2, tail)\n",
        "        # upsampling\n",
        "        cx1 = torch.nn.functional.interpolate(cx1, size=sx.size()[-2:], mode='bilinear')\n",
        "        cx2 = torch.nn.functional.interpolate(cx2, size=sx.size()[-2:], mode='bilinear')\n",
        "        cx = torch.cat((cx1, cx2), dim=1)\n",
        "\n",
        "        if self.training == True:\n",
        "            cx1_sup = self.supervision1(cx1)\n",
        "            cx2_sup = self.supervision2(cx2)\n",
        "            cx1_sup = torch.nn.functional.interpolate(cx1_sup, size=input.size()[-2:], mode='bilinear')\n",
        "            cx2_sup = torch.nn.functional.interpolate(cx2_sup, size=input.size()[-2:], mode='bilinear')\n",
        "\n",
        "        # output of feature fusion module\n",
        "        result = self.feature_fusion_module(sx, cx)\n",
        "\n",
        "        # upsampling\n",
        "        result = torch.nn.functional.interpolate(result, scale_factor=8, mode='bilinear')\n",
        "        result = self.conv(result)\n",
        "\n",
        "        if self.training == True:\n",
        "            return result, cx1_sup, cx2_sup\n",
        "\n",
        "        return result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_isSfs1sgJGE"
      },
      "source": [
        "# Wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PLkzB48ZQqC"
      },
      "outputs": [],
      "source": [
        "!pip -q install wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9ffq97eM8Rg"
      },
      "source": [
        "# Train/Val loops"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0KuuGIWNAbF"
      },
      "source": [
        "### Train Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-slUs8vsNAEe"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "import wandb\n",
        "\n",
        "def train(model:nn.Module, source_loader:DataLoader, target_loader:DataLoader, criterion:nn.Module, optimizer:optim.Optimizer, classmixer:ClassMixer) -> tuple[float, float, torch.Tensor, torch.Tensor]:\n",
        "    global device\n",
        "    global n_classes\n",
        "    global ENABLE_PRINT\n",
        "    global ENABLE_WANDB_LOG\n",
        "    global train_step\n",
        "    global log_per_epoch\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    num_batch = len(source_loader)\n",
        "    chunk_batch = num_batch//log_per_epoch+1\n",
        "\n",
        "    num_sample = len(source_loader.dataset)\n",
        "    seen_sample = 0\n",
        "\n",
        "    train_loss = 0.0\n",
        "    train_hist = torch.zeros((n_classes,n_classes), device=device)\n",
        "\n",
        "    batch_idx = 0\n",
        "    \n",
        "    # TODO: train1, train2, ... one for each step instead of a unified one?\n",
        "    # NOTE: assume inputs_src.size(0) / inputs_target.size(0) = 2 \n",
        "    # NOTE: threshold = 0.968, from https://github.com/vikolss/DACS\n",
        "    for (inputs_src, _, label_source), (inputs_target, _, _) in zip(source_loader, target_loader):\n",
        "        B = inputs_src.size(0)\n",
        "        seen_sample += B\n",
        "\n",
        "        inputs_src, label_source = inputs_src.to(device), label_source.squeeze().to(device)\n",
        "        \n",
        "        classmixer.create_mask(label_source[B//2:])\n",
        "        inputs_mixed = classmixer.mix_img(inputs_src[B//2:], inputs_target).to(device)\n",
        "\n",
        "        outputs_source, cx1_sup, cx2_sup = model(inputs_src[:B//2])\n",
        "        outputs_mixed, cx3_sup, cx4_sup = model(inputs_mixed)\n",
        "        \n",
        "        # Loss calculation:\n",
        "        max_probs, target_pseudo_labels = torch.softmax(outputs_mixed, dim=1).max()\n",
        "\n",
        "        label_mixed = classmixer.mix_label(label_source[B//2:], target_pseudo_labels)\n",
        "    \n",
        "        l1 = criterion(outputs_source, label_source) + criterion(cx1_sup, label_source) + criterion(cx2_sup, label_source)\n",
        "        l2 = criterion(outputs_mixed, label_mixed) + criterion(cx3_sup, label_mixed) + criterion(cx4_sup, label_mixed)\n",
        "\n",
        "        a, b, c = max_probs.size()\n",
        "        lambda_ = torch.sum(max_probs.ge(0.968)).item() / (a*b*c) \n",
        "        \n",
        "        loss = l1 + lambda_*l2\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        predicted_source = outputs_source.argmax(1)\n",
        "        predicted_mixed = outputs_mixed.argmax(1)\n",
        "\n",
        "        hist_batch = torch.zeros((n_classes, n_classes), device=device)\n",
        "\n",
        "        # TODO: concat label and prediction for easier iteration\n",
        "        for y_pred, y in zip(torch.cat(predicted_source, predicted_mixed), torch.cat(label_source, label_mixed)):\n",
        "            hist_batch += fast_hist_torch_cuda(y_pred.detach(), y.detach(), n_classes)\n",
        "\n",
        "        train_loss += loss.item() * B\n",
        "        train_hist += hist_batch\n",
        "\n",
        "        if ((batch_idx+1) % chunk_batch) == 0:\n",
        "            iou_batch = per_class_iou_cuda(hist_batch)\n",
        "            if ENABLE_PRINT:\n",
        "                    print(f'Training [{seen_sample}/{num_sample} ({100. * seen_sample / num_sample:.0f}%)]')\n",
        "                    print(f'\\tLoss: {loss.item():.6f}')\n",
        "                    print(f\"\\tmIoU: {100.*iou_batch[iou_batch > 0].mean():.4f}\")\n",
        "\n",
        "            if ENABLE_WANDB_LOG:\n",
        "                wandb.log({\n",
        "                        \"train/step\": train_step,\n",
        "                        \"train/batch_loss\": loss.item(),\n",
        "                        \"train/batch_mIou\": 100.*iou_batch[iou_batch > 0].mean()\n",
        "                    },\n",
        "                    commit=True,\n",
        "                )\n",
        "                train_step += 1\n",
        "        batch_idx += 1\n",
        "\n",
        "    train_loss = train_loss / seen_sample\n",
        "\n",
        "    train_iou_class = per_class_iou_cuda(train_hist)\n",
        "    train_mIou = train_iou_class[train_iou_class > 0].mean()\n",
        "\n",
        "    return train_loss, train_mIou.item(), train_hist, train_iou_class\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ON-XdXCkNCrn"
      },
      "source": [
        "### Validation loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13a6MV00QIot"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import wandb\n",
        "\n",
        "def validate(model:nn.Module, val_loader:DataLoader, criterion:nn.Module) -> tuple[float, float, torch.Tensor, torch.Tensor]:\n",
        "    global device\n",
        "    global n_classes\n",
        "    global ENABLE_PRINT\n",
        "    global ENABLE_WANDB_LOG\n",
        "    global val_step\n",
        "    global log_per_epoch\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    num_batch = len(val_loader)\n",
        "    chunk_batch = num_batch//log_per_epoch+1\n",
        "\n",
        "    num_sample = len(val_loader.dataset)\n",
        "    seen_sample = 0\n",
        "    chunk_sample = 0\n",
        "\n",
        "    val_loss = 0.0\n",
        "    val_hist = torch.zeros((n_classes,n_classes)).to(device)\n",
        "\n",
        "    chunk_loss = 0.0\n",
        "    chunk_hist = torch.zeros((n_classes,n_classes)).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, _, targets) in enumerate(val_loader):\n",
        "            batch_size = inputs.size(0)\n",
        "\n",
        "            inputs, targets = inputs.to(device), targets.squeeze().to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            predicted = outputs.argmax(1)\n",
        "\n",
        "            hist_batch = torch.zeros((n_classes, n_classes)).to(device)\n",
        "            for i in range(len(inputs)):\n",
        "                hist_batch += fast_hist_torch_cuda(targets[i].detach(), predicted[i].detach(), n_classes)\n",
        "\n",
        "            chunk_sample += batch_size\n",
        "            chunk_loss += loss.item() * batch_size\n",
        "            chunk_hist += hist_batch\n",
        "\n",
        "            if ((batch_idx+1) % chunk_batch) == 0:\n",
        "                seen_sample += chunk_sample\n",
        "                val_loss += chunk_loss\n",
        "                val_hist += chunk_hist\n",
        "\n",
        "                if ENABLE_PRINT:\n",
        "                    iou_batch = per_class_iou_cuda(hist_batch)\n",
        "                    print(f'Validation [{seen_sample}/{num_sample} ({100. * seen_sample / num_sample:.0f}%)]')\n",
        "                    print(f'\\tLoss: {loss.item():.6f}')\n",
        "                    print(f\"\\tmIoU: {100.*iou_batch[iou_batch > 0].mean():.4f}\")\n",
        "\n",
        "                if ENABLE_WANDB_LOG:\n",
        "                    iou_batch = per_class_iou_cuda(chunk_hist)\n",
        "                    wandb.log({\n",
        "                            \"validate/step\": val_step,\n",
        "                            \"validate/batch_loss\": chunk_loss/chunk_sample,\n",
        "                            \"validate/batch_mIou\": 100.*iou_batch[iou_batch > 0].mean()\n",
        "                        },\n",
        "                        commit=True,\n",
        "                    )\n",
        "\n",
        "                    val_step += 1\n",
        "\n",
        "                chunk_sample = 0\n",
        "                chunk_loss = 0.0\n",
        "                chunk_hist = torch.zeros((n_classes, n_classes)).to(device)\n",
        "\n",
        "        if chunk_sample > 0:\n",
        "            seen_sample += chunk_sample\n",
        "            val_loss += chunk_loss\n",
        "            val_hist += chunk_hist\n",
        "\n",
        "            if ENABLE_PRINT:\n",
        "                iou_batch = per_class_iou_cuda(hist_batch)\n",
        "                print(f'Validation [{seen_sample}/{num_sample} ({100. * seen_sample / num_sample:.0f}%)]')\n",
        "                print(f'\\tLoss: {loss.item():.6f}')\n",
        "                print(f\"\\tmIoU: {100.*iou_batch[iou_batch > 0].mean():.4f}\")\n",
        "\n",
        "            if ENABLE_WANDB_LOG:\n",
        "                iou_batch = per_class_iou_cuda(chunk_hist)\n",
        "                wandb.log({\n",
        "                        \"validate/step\": val_step,\n",
        "                        \"validate/batch_loss\": chunk_loss/chunk_sample,\n",
        "                        \"validate/batch_mIou\": 100.*iou_batch[iou_batch > 0].mean()\n",
        "                    },\n",
        "                    commit=True,\n",
        "                )\n",
        "\n",
        "                val_step += 1\n",
        "\n",
        "    val_loss = val_loss / seen_sample\n",
        "\n",
        "    val_iou_class = per_class_iou_cuda(val_hist)\n",
        "    val_mIou = val_iou_class[val_iou_class > 0].mean()\n",
        "\n",
        "    return val_loss, val_mIou, val_hist, val_iou_class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgPi7OfDNLJX"
      },
      "source": [
        "# Machine learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "QH9ZSG-m617c"
      },
      "outputs": [],
      "source": [
        "def pipeline():\n",
        "    from torch.utils.data import DataLoader\n",
        "    import torchvision.transforms as TF\n",
        "    import torch.nn as nn\n",
        "    import torch.optim as optim\n",
        "    import wandb\n",
        "    import os\n",
        "\n",
        "    global device\n",
        "    global n_classes\n",
        "    global ENABLE_PRINT\n",
        "    global ENABLE_WANDB_LOG\n",
        "    global train_step\n",
        "    global val_step\n",
        "    global log_per_epoch\n",
        "\n",
        "    ENABLE_PRINT = False\n",
        "    ENABLE_WANDB_LOG = True\n",
        "    train_step = 0\n",
        "    val_step = 0\n",
        "    log_per_epoch = 20\n",
        "\n",
        "    models_root_dir = \"./models\"\n",
        "    !rm -rf {models_root_dir}\n",
        "    !mkdir {models_root_dir}\n",
        "\n",
        "    B = 3\n",
        "    n_classes = 19\n",
        "\n",
        "    backbone = \"BiSeNet\"\n",
        "    context_path = \"resnet18\"\n",
        "\n",
        "    start_epoch = 0 # <--------- Last epoch that i completed (in this script i will perform from the start+1 to end)\n",
        "    end_epoch = 2\n",
        "    max_epoch = 50\n",
        "\n",
        "    assert start_epoch < end_epoch <= max_epoch, \"Check your start/end/max epoch settings.\"\n",
        "\n",
        "    init_lr=2.5e-4\n",
        "    lr_decay_iter = 1\n",
        "    momentum=0.9\n",
        "    weight_decay=1e-4\n",
        "    dataset = \"GTA5_FDA\"\n",
        "\n",
        "    target_size = (512, 1024)\n",
        "    source_size = (780, 1280)\n",
        "\n",
        "\n",
        "    img_transform_target = TF.Compose([\n",
        "        TF.ToTensor(),\n",
        "        TF.Resize(target_size),\n",
        "        TF.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                    std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    \n",
        "    img_transform_source = TF.Compose([\n",
        "        TF.ToTensor(),\n",
        "        TF.Resize(source_size),\n",
        "        TF.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                    std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    \n",
        "    label_transform_target = TF.Compose([\n",
        "        TF.ToTensor(),\n",
        "        TF.Resize((H, W), interpolation=TF.InterpolationMode.NEAREST),\n",
        "    ])    \n",
        "    label_transform_source = TF.Compose([\n",
        "        TF.ToTensor(),\n",
        "        TF.Resize((H, W), interpolation=TF.InterpolationMode.NEAREST),\n",
        "    ])\n",
        "\n",
        "    # Dataset objects\n",
        "    if dataset == \"mixed\":\n",
        "        city_val = CityScapes(\"./Cityscapes/Cityspaces\", split=\"train\", transform=img_transform_target, target_transform=label_transform_target)\n",
        "        city_train = CityScapes(\"./Cityscapes/Cityspaces\", split=\"val\", transform=img_transform_target, target_transform=label_transform_target)\n",
        "        gta_train, _ = GTA5_dataset_splitter(\"./GTA_extended\", 1.0, None, transform=img_transform_source, target_transform=label_transform_source)\n",
        "\n",
        "        source_loader = DataLoader(gta_train, batch_size=B, shuffle=True)\n",
        "        target_loader = DataLoader(city_train, batch_size=B//2, shuffle=True)\n",
        "        val_loader = DataLoader(city_val, batch_size=B, shuffle=True)\n",
        "    else:\n",
        "        raise Exception(\"Wrong dataset name\")\n",
        "    \n",
        "    # Architecture\n",
        "    if backbone == \"BiSeNet\":\n",
        "        model = BiSeNet(n_classes, context_path).to(device)\n",
        "        architecture = backbone+\"-\"+context_path\n",
        "    else:\n",
        "        raise Exception(\"Wrong model name\")\n",
        "\n",
        "    # The other 2\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=255)\n",
        "    optimizer = optim.SGD(model.parameters(), lr=init_lr, momentum=momentum, weight_decay=weight_decay)\n",
        "\n",
        "    # Wandb setup and metrics\n",
        "    run_name = f\"step_4B_DACS\"\n",
        "    run_detail = f\"{run_name}_{architecture}_{dataset}\"\n",
        "    run_id = None \n",
        "\n",
        "    run = wandb.init(\n",
        "        entity=\"Machine_learning_and_Deep_learning_labs\",\n",
        "        project=\"Semantic Segmentation\",\n",
        "        name=run_name,\n",
        "        # id=run_id,\n",
        "        resume=\"allow\", # <----------------  IMPORTANT CONFIG KEY\n",
        "        config={\n",
        "            \"initial_learning_rate\": init_lr,\n",
        "            \"lr_decay_iter\": lr_decay_iter,\n",
        "            \"momentum\": momentum,\n",
        "            \"weight_decay\": weight_decay,\n",
        "            \"architecture\": architecture,\n",
        "            \"dataset\": dataset,\n",
        "            \"start_epoch\": start_epoch,\n",
        "            \"end_epoch\": end_epoch,\n",
        "            \"max_epoch\": max_epoch,\n",
        "            \"batch\": B,\n",
        "            \"lr_scheduler\": \"poly\"\n",
        "        },\n",
        "    )\n",
        "    if run_id is None:\n",
        "        print(f\"\\nThe id of this run is {run.id}\\n\")\n",
        "        \n",
        "    wandb.define_metric(\"epoch/step\")\n",
        "    wandb.define_metric(\"epoch/*\", step_metric=\"epoch/step\")\n",
        "\n",
        "    wandb.define_metric(\"train/step\")\n",
        "    wandb.define_metric(\"train/*\", step_metric=\"train/step\")\n",
        "\n",
        "    wandb.define_metric(\"validate/step\")\n",
        "    wandb.define_metric(\"validate/*\", step_metric=\"validate/step\")\n",
        "\n",
        "    # Loading form a starting point\n",
        "    if start_epoch > 0:\n",
        "        artifact = run.use_artifact(f'Machine_learning_and_Deep_learning_labs/Semantic Segmentation/{run_id}:epoch_{start_epoch}', type='model')\n",
        "        artifact_dir = artifact.download()\n",
        "\n",
        "        artifact_path = os.path.join(artifact_dir, run_detail+f\"_epoch_{start_epoch}.pth\")\n",
        "\n",
        "        checkpoint = torch.load(artifact_path, map_location=device)\n",
        "\n",
        "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
        "\n",
        "        train_step = checkpoint[\"train_step\"]+1\n",
        "        val_step = checkpoint[\"validate_step\"]+1\n",
        "\n",
        "    # Main Loop\n",
        "    for epoch in range(start_epoch+1, end_epoch+1):\n",
        "        print(\"-----------------------------\")\n",
        "        print(f\"Epoch {epoch}\")\n",
        "\n",
        "        lr = poly_lr_scheduler(optimizer, init_lr, epoch-1, max_iter=max_epoch)\n",
        "\n",
        "        print(f\"[Poly LR] 100xLR: {100.*lr:.6f}\")\n",
        "\n",
        "        run.log({\n",
        "            \"epoch/step\": epoch,\n",
        "            \"epoch/100xlearning_rate\": 100.*lr,\n",
        "        })\n",
        "\n",
        "        train_loss, train_mIou, train_hist, train_mIou_class = train(model, source_loader, target_loader, criterion, optimizer)\n",
        "\n",
        "        print(f'[Train Loss] : {train_loss:.6f} [mIoU]: {100.*train_mIou:.2f}%')\n",
        "\n",
        "        run.log({\n",
        "                \"epoch/step\": epoch,\n",
        "                \"epoch/train_loss\": train_loss,\n",
        "                \"epoch/train_mIou\": 100*train_mIou\n",
        "            },\n",
        "            commit=True,\n",
        "        )\n",
        "\n",
        "        val_loss, val_mIou, val_hist, val_mIou_class = validate(model, val_loader, criterion)\n",
        "\n",
        "        print(f'[Validation Loss] : {val_loss:.6f} [mIoU]: {100.*val_mIou:.2f}%')\n",
        "\n",
        "        run.log({\n",
        "                \"epoch/step\": epoch,\n",
        "                \"epoch/val_loss\": val_loss,\n",
        "                \"epoch/val_mIou\": 100*val_mIou\n",
        "            },\n",
        "            commit=True\n",
        "        )\n",
        "\n",
        "        if epoch % 2 == 0 or epoch == end_epoch:\n",
        "            checkpoint = {\n",
        "                \"model_state_dict\": model.state_dict(),\n",
        "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "                \"train_step\": train_step,\n",
        "                \"validate_step\": val_step,\n",
        "            }\n",
        "\n",
        "            file_name = f\"{run_id}_epoch_{epoch}.pth\"\n",
        "\n",
        "            # Saving the progress\n",
        "            file_path = os.path.join(models_root_dir, file_name)\n",
        "            torch.save(checkpoint, file_path)\n",
        "\n",
        "            print(f\"Model saved to {file_path}\")\n",
        "\n",
        "            artifact = wandb.Artifact(name=run_id, type=\"model\")\n",
        "            artifact.add_file(file_path)\n",
        "\n",
        "            run.log_artifact(artifact, aliases=[\"latest\", f\"epoch_{epoch}\"])\n",
        "\n",
        "        if (epoch % 10) == 0:\n",
        "            log_confusion_matrix(\"Confusion Matrix - Train\", train_hist.cpu().numpy(), \"epoch/train_confusion_matrix\", \"epoch/step\", epoch)\n",
        "            log_confusion_matrix(\"Confusion Matrix - Validate\", val_hist.cpu().numpy(), \"epoch/validate_confusion_matrix\", \"epoch/step\", epoch)\n",
        "\n",
        "            log_bar_chart_ioU(f\"Train IoU per class - epoch {epoch}\", [c.name for c in GTA5Labels_TaskCV2017().list_ if c.name != \"void\"], train_mIou, train_mIou_class.cpu().numpy(), \"epoch/train_Iou_class\", \"epoch/step\", epoch)\n",
        "            log_bar_chart_ioU(f\"Validate IoU per class - epoch {epoch}\", [c.name for c in GTA5Labels_TaskCV2017().list_ if c.name != \"void\"], val_mIou, val_mIou_class.cpu().numpy(), \"epoch/validate_Iou_class\", \"epoch/step\", epoch)\n",
        "\n",
        "    run.config[\"end_epoch\"] = end_epoch\n",
        "    run.config[\"start_epoch\"] = start_epoch\n",
        "\n",
        "    if end_epoch == max_epoch:\n",
        "        mean_latency, std_latency, mean_fps = latency(device, model, H=512, W=1024)\n",
        "\n",
        "        run.log({\n",
        "            \"model/flops\": num_flops(device, model, 512, 1024),\n",
        "            \"model/latency_mean\": mean_latency,\n",
        "            \"model/latency_std\": std_latency,\n",
        "            \"model/mean_fps\": mean_fps,\n",
        "            \"model/param\": num_param(model)\n",
        "        })\n",
        "\n",
        "    run.finish()\n",
        "\n",
        "# wandb.finish()\n",
        "pipeline()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "b5n1Dm1A0gjl",
        "Jntcuf0zNwaq",
        "c9TMCaEmg1N4",
        "JxswDJZOmLqQ",
        "gEx91uPqlFv3",
        "LTki3m7aNH10",
        "O2ybDTYzv0ZS",
        "XPTyvGv0fQd8",
        "_isSfs1sgJGE",
        "w0KuuGIWNAbF",
        "ON-XdXCkNCrn",
        "rgPi7OfDNLJX"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "052c813485864162906ce72faede3370": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "07533cefef9147be90b61bff6e94280b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c16b236d7dc74c999529c30b6abdf89d",
            "placeholder": "​",
            "style": "IPY_MODEL_bf7d292d791e4d3ea70e445856e35744",
            "value": " 3355/12432 [00:24&lt;00:53, 168.18it/s]"
          }
        },
        "0e449ad830c34088b31d0cf60c6cf224": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_589369f18fe24a8b994f1a53fdeda49a",
            "placeholder": "​",
            "style": "IPY_MODEL_ea2cfa72ef024c55b0feb63bf5d40e31",
            "value": "Extracting PNGs:  27%"
          }
        },
        "249fb549c78d48c58ad588a3596f3825": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0e449ad830c34088b31d0cf60c6cf224",
              "IPY_MODEL_de610e2099e245dd80b6fbd6b39da0fe",
              "IPY_MODEL_07533cefef9147be90b61bff6e94280b"
            ],
            "layout": "IPY_MODEL_6aa9795250e74752acc7a342762a0972"
          }
        },
        "589369f18fe24a8b994f1a53fdeda49a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6aa9795250e74752acc7a342762a0972": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae13707e46444778addfbb572b2efbe7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bf7d292d791e4d3ea70e445856e35744": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c16b236d7dc74c999529c30b6abdf89d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de610e2099e245dd80b6fbd6b39da0fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_052c813485864162906ce72faede3370",
            "max": 12432,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ae13707e46444778addfbb572b2efbe7",
            "value": 3355
          }
        },
        "ea2cfa72ef024c55b0feb63bf5d40e31": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
