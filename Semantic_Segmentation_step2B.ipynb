{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWIvDpquM-Tl"
      },
      "source": [
        "# Preparations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ia5-WeZ0op3"
      },
      "source": [
        "## Variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5n1Dm1A0gjl"
      },
      "source": [
        "### Logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_HX9maPV0iVa"
      },
      "outputs": [],
      "source": [
        "ENABLE_PRINT = False\n",
        "ENABLE_WANDB_LOG = True\n",
        "log_per_epoch = 20\n",
        "n_classes = 19\n",
        "\n",
        "train_step = 0\n",
        "val_step = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jntcuf0zNwaq"
      },
      "source": [
        "### Device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ie8Own_ANu_t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1543bfc7-1a6b-4b17-bd65-594fd3838931"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPZY7dOBM6_m"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9TMCaEmg1N4"
      },
      "source": [
        "### Label-color correlator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZjUPxdZcbc-N"
      },
      "outputs": [],
      "source": [
        "from abc import ABCMeta\n",
        "from dataclasses import dataclass\n",
        "from typing import Tuple\n",
        "\n",
        "@dataclass\n",
        "class GTA5Label:\n",
        "    name: str\n",
        "    ID: int\n",
        "    color: Tuple[int, int, int]\n",
        "\n",
        "class GTA5Labels_TaskCV2017():\n",
        "    road = GTA5Label(name = \"road\", ID=0, color=(128, 64, 128))\n",
        "    sidewalk = GTA5Label(name = \"sidewalk\", ID=1, color=(244, 35, 232))\n",
        "    building = GTA5Label(name = \"building\", ID=2, color=(70, 70, 70))\n",
        "    wall = GTA5Label(name = \"wall\", ID=3, color=(102, 102, 156))\n",
        "    fence = GTA5Label(name = \"fence\", ID=4, color=(190, 153, 153))\n",
        "    pole = GTA5Label(name = \"pole\", ID=5, color=(153, 153, 153))\n",
        "    light = GTA5Label(name = \"light\", ID=6, color=(250, 170, 30))\n",
        "    sign = GTA5Label(name = \"sign\", ID=7, color=(220, 220, 0))\n",
        "    vegetation = GTA5Label(name = \"vegetation\", ID=8, color=(107, 142, 35))\n",
        "    terrain = GTA5Label(name = \"terrain\", ID=9, color=(152, 251, 152))\n",
        "    sky = GTA5Label(name = \"sky\", ID=10, color=(70, 130, 180))\n",
        "    person = GTA5Label(name = \"person\", ID=11, color=(220, 20, 60))\n",
        "    rider = GTA5Label(name = \"rider\", ID=12, color=(255, 0, 0))\n",
        "    car = GTA5Label(name = \"car\", ID=13, color=(0, 0, 142))\n",
        "    truck = GTA5Label(name = \"truck\", ID=14, color=(0, 0, 70))\n",
        "    bus = GTA5Label(name = \"bus\", ID=15, color=(0, 60, 100))\n",
        "    train = GTA5Label(name = \"train\", ID=16, color=(0, 80, 100))\n",
        "    motocycle = GTA5Label(name = \"motocycle\", ID=17, color=(0, 0, 230))\n",
        "    bicycle = GTA5Label(name = \"bicycle\", ID=18, color=(119, 11, 32))\n",
        "    void = GTA5Label(name = \"void\", ID=255, color=(0,0,0))\n",
        "\n",
        "    list_ = [\n",
        "        road,\n",
        "        sidewalk,\n",
        "        building,\n",
        "        wall,\n",
        "        fence,\n",
        "        pole,\n",
        "        light,\n",
        "        sign,\n",
        "        vegetation,\n",
        "        terrain,\n",
        "        sky,\n",
        "        person,\n",
        "        rider,\n",
        "        car,\n",
        "        truck,\n",
        "        bus,\n",
        "        train,\n",
        "        motocycle,\n",
        "        bicycle,\n",
        "        void\n",
        "    ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxswDJZOmLqQ"
      },
      "source": [
        "### Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-fjLlz9y2jfn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import wandb\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import zipfile\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import time\n",
        "\n",
        "!pip -q install -U fvcore\n",
        "\n",
        "from fvcore.nn import FlopCountAnalysis, flop_count_table\n",
        "\n",
        "def pretty_extract(zip_path:str, extract_to:str) -> None:\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        png_files = [f for f in zip_ref.namelist() if f.endswith('.png')]\n",
        "\n",
        "        for file in tqdm(png_files, desc=\"Extracting PNGs\"):\n",
        "            zip_ref.extract(file, path=extract_to)\n",
        "\n",
        "def poly_lr_scheduler(optimizer, init_lr:float, iter:int=0, lr_decay_iter:int=1, max_iter:int=50, power:float=0.9) -> float:\n",
        "    \"\"\"Polynomial decay of learning rate\n",
        "            :param init_lr is base learning rate\n",
        "            :param iter is a current iteration\n",
        "            :param lr_decay_iter how frequently decay occurs, default is 1\n",
        "            :param max_iter is number of maximum iterations\n",
        "            :param power is a polymomial power\n",
        "\n",
        "    \"\"\"\n",
        "    if ((iter % lr_decay_iter) != 0) or iter > max_iter:\n",
        "        return optimizer.param_groups[0]['lr']\n",
        "\n",
        "    lr = init_lr*(1 - iter/max_iter)**power\n",
        "    optimizer.param_groups[0]['lr'] = lr\n",
        "    return lr\n",
        "\n",
        "def fast_hist(a:np.ndarray, b:np.ndarray, n:int) -> np.ndarray:\n",
        "    '''\n",
        "    a and b are label and prediction respectively\n",
        "    n is the number of classes\n",
        "    '''\n",
        "    k = (a >= 0) & (a < n)\n",
        "    return np.bincount(n * a[k].astype(int) + b[k], minlength=n ** 2).reshape(n, n)\n",
        "\n",
        "def fast_hist_torch_cuda(a: torch.Tensor, b: torch.Tensor, n: int) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    a and b are label and prediction respectively.\n",
        "    n is the number of classes.\n",
        "    This version works with CUDA tensors.\n",
        "    \"\"\"\n",
        "    k = (a >= 0) & (a < n)\n",
        "    a = a[k].to(torch.int64)\n",
        "    b = b[k].to(torch.int64)\n",
        "\n",
        "    # Compute linear indices\n",
        "    indices = n * a + b\n",
        "\n",
        "    # Initialize histogram on the same device\n",
        "    hist = torch.zeros(n * n, dtype=torch.int64, device=a.device)\n",
        "\n",
        "    # Count occurrences (equivalent to bincount)\n",
        "    hist.scatter_add_(0, indices, torch.ones_like(indices, dtype=torch.int64))\n",
        "\n",
        "    return hist.view(n, n)\n",
        "\n",
        "def per_class_iou(hist:np.ndarray) -> np.ndarray:\n",
        "    epsilon = 1e-5\n",
        "    return (np.diag(hist)) / (hist.sum(1) + hist.sum(0) - np.diag(hist) + epsilon)\n",
        "\n",
        "def per_class_iou_cuda(hist:torch.Tensor) -> torch.Tensor:\n",
        "    epsilon = 1e-5\n",
        "    diag = torch.diag(hist)\n",
        "\n",
        "    sum_rows = hist.sum(dim=1)\n",
        "    sum_cols = hist.sum(dim=0)\n",
        "\n",
        "    iou = diag / (sum_rows + sum_cols - diag + epsilon)\n",
        "    return iou\n",
        "\n",
        "# Mapping labelId image to RGB image\n",
        "def decode_segmap(mask:np.ndarray) -> np.ndarray:\n",
        "    h, w = mask.shape\n",
        "    color_mask = np.zeros((h, w, 3), dtype=np.uint8)\n",
        "    for label_id in GTA5Labels_TaskCV2017().list_:\n",
        "        color_mask[mask == label_id.ID, :] = label_id.color\n",
        "\n",
        "    return color_mask\n",
        "\n",
        "def tensorToImageCompatible(t:torch.Tensor) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    convert from a tensor of shape [C, H, W] where a normalization has been applied\n",
        "    to an unnormalized tensor of shape [H, W, C],\n",
        "    so *plt.imshow(tensorToImageCompatible(tensor))* works as expected.\\n\n",
        "    Intended to be used to recover the original element\n",
        "    when this transformation is used:\n",
        "    - transform = TF.Compose([\n",
        "        TF.ToTensor(),\n",
        "        TF.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                    std=[0.229, 0.224, 0.225])])\n",
        "    \"\"\"\n",
        "    mean = torch.tensor([0.485, 0.456, 0.406]).view([-1, 1, 1])\n",
        "    std = torch.tensor([0.229, 0.224, 0.225]).view([-1, 1, 1])\n",
        "\n",
        "    unnormalized = t * std + mean\n",
        "\n",
        "    return (unnormalized.permute(1,2,0).clamp(0,1).numpy()*255).astype(np.uint8)\n",
        "\n",
        "\n",
        "def log_confusion_matrix(title:str, hist:np.ndarray, tag:str, step_name:str, step_value:int):\n",
        "    row_sums = hist.sum(axis=1, keepdims=True)\n",
        "    safe_hist = np.where(row_sums == 0, 0, hist / row_sums)\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(100.*safe_hist, fmt=\".2f\", annot=True, cmap=\"Blues\", annot_kws={'size': 7})\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"True\")\n",
        "    plt.title(title)\n",
        "\n",
        "    wandb.log({tag: wandb.Image(plt), step_name:step_value})\n",
        "    plt.close()\n",
        "\n",
        "def log_confusion_matrix_cuda(title:str, hist:torch.Tensor, tag:str, step_name:str, step_value:int):\n",
        "    hist_np = hist.detach().cpu().numpy()  # Conversione per compatibilità con seaborn\n",
        "\n",
        "    row_sums = hist.sum(axis=1, keepdims=True)\n",
        "    safe_hist = np.where(row_sums == 0, 0, hist / row_sums)\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(100.*safe_hist, fmt=\".2f\", annot=True, cmap=\"Blues\", annot_kws={'size': 7})\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"True\")\n",
        "    plt.title(title)\n",
        "\n",
        "    wandb.log({tag: wandb.Image(plt), step_name:step_value})\n",
        "    plt.close()\n",
        "\n",
        "def log_bar_chart_ioU(title:str, class_names:list, mIou:float, iou_class:list, tag:str, step_name:str, epoch:int):\n",
        "    iou_percent = [round(iou*100., 2) for iou in iou_class]\n",
        "    miou_percent = round(mIou*100., 2)\n",
        "\n",
        "    all_labels = [\"mIoU\"] + class_names\n",
        "    all_values = [miou_percent] + iou_percent\n",
        "\n",
        "    plt.figure(figsize=(14, 5))\n",
        "    bars = plt.bar(range(len(all_values)), all_values, color='skyblue')\n",
        "    plt.xticks(range(len(all_labels)), all_labels, rotation=45, ha=\"right\")\n",
        "\n",
        "    for i, bar in enumerate(bars):\n",
        "        height = bar.get_height()\n",
        "        plt.text(bar.get_x() + bar.get_width() / 2.0, height + 1, f'{height:.2f}',\n",
        "                ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "    plt.ylabel(\"IoU (%)\")\n",
        "    plt.ylim(0, 105)\n",
        "    plt.title(title)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    wandb.log({tag: wandb.Image(plt), step_name:epoch})\n",
        "\n",
        "    plt.close()\n",
        "\n",
        "def num_flops(device, model:torch.nn.Module, H:int, W:int):\n",
        "    model.eval()\n",
        "    img = (torch.zeros((1,3,H,W), device=device),)\n",
        "\n",
        "    flops = FlopCountAnalysis(model, img)\n",
        "    # return flop_count_table(flops)\n",
        "    # return flops.total()\n",
        "\n",
        "    flops = FlopCountAnalysis(model, (torch.zeros((1, 3, 512, 1024), device=device),))\n",
        "    flops_by_op = flops.by_operator()\n",
        "\n",
        "    # Prendi la prima operazione (es: 'conv2d') e i suoi flops\n",
        "    first_op_name = list(flops_by_op.keys())[0]\n",
        "    first_op_flops = flops_by_op[first_op_name] / 1e9  # in GFLOPs\n",
        "\n",
        "def latency(device, model:torch.nn.Module, H:int, W:int):\n",
        "    model.eval()\n",
        "\n",
        "    img = torch.zeros((1,3,H,W)).to(device)\n",
        "    iterations = 100\n",
        "    latency_list = []\n",
        "    FPS_list  = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in tqdm(range(iterations)):\n",
        "            start_time = time.time()\n",
        "            _ = model(img)\n",
        "            end_time = time.time()\n",
        "\n",
        "            latency = end_time - start_time\n",
        "\n",
        "            latency_list.append(latency)\n",
        "            FPS_list.append(1.0/latency)\n",
        "\n",
        "    mean_latency = np.mean(latency_list)*1000\n",
        "    std_latency = np.std(latency_list)*1000\n",
        "    mean_FPS = np.mean(FPS_list)\n",
        "\n",
        "    return mean_latency, std_latency, mean_FPS\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLfNdNzyMyxn"
      },
      "source": [
        "## CityScapes download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134,
          "referenced_widgets": [
            "6b05864ee44849cf87e9c44b24b5eeeb",
            "e1f088c5729c4a9c91f28438667278e7",
            "173cbbd900da41b1bc87a5da64855951",
            "dbb3b93d9f6e4c17955f5dfe86b3e9bd",
            "0d2ae9857a2a4865b4ade50dffe2434f",
            "827ce8d8d695468c851a6dbd8f853ef8",
            "304a2517f3d340278caf28fd6ce4444e",
            "39d61cc6255346a5885d7fd6ffcf3c17",
            "8a1f980960754f7fa67c3509e597710b",
            "e6bf5c1f6e984649a3beb0ab2e98891e",
            "e1337057ee17487ab79d2302d2150bdc"
          ]
        },
        "id": "B5p-mIhc2uIx",
        "outputId": "8cbc06f8-de62-4815-d2e3-cf70009858b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1MI8QsvjW0R6WDJiL49L7sDGpPWYAQB6O\n",
            "From (redirected): https://drive.google.com/uc?id=1MI8QsvjW0R6WDJiL49L7sDGpPWYAQB6O&confirm=t&uuid=583a3567-d0bc-4404-8093-d0ce47cb55ca\n",
            "To: /content/Cityscapes.zip\n",
            "100% 4.97G/4.97G [00:44<00:00, 111MB/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Extracting PNGs:   0%|          | 0/6216 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6b05864ee44849cf87e9c44b24b5eeeb"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install -q gdown\n",
        "\n",
        "file_id = \"1MI8QsvjW0R6WDJiL49L7sDGpPWYAQB6O\"\n",
        "!gdown https://drive.google.com/uc?id={file_id}\n",
        "\n",
        "pretty_extract(\"Cityscapes.zip\", \".\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZME_xiNNE-g"
      },
      "source": [
        "## Cityscapes implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRCoySxr2fnu"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "import os\n",
        "# from torchvision.transforms import ToTensor, ToPILImage\n",
        "import cv2\n",
        "from PIL import Image\n",
        "\n",
        "# from torchvision.io import decode_image, read_image\n",
        "# from torchvision.io.image import ImageReadMode\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class CityScapes(Dataset):\n",
        "    def __init__(self, rootdir, split=\"train\", targetdir=\"gtFine\", imgdir=\"images\", transform=None, target_transform=None):\n",
        "        super(CityScapes, self).__init__()\n",
        "\n",
        "        self.rootdir = rootdir\n",
        "        self.split = split\n",
        "        self.targetdir = os.path.join(self.rootdir, targetdir, self.split) # ./gtFine/train/\n",
        "        self.imgdir = os.path.join(self.rootdir, imgdir, self.split) # ./images/train/\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "        self.imgs_path = []\n",
        "        self.targets_color_path = []\n",
        "        self.targets_labelIds_path = []\n",
        "\n",
        "        for city in os.listdir(self.imgdir): # frankfurt\n",
        "            img_city_dir = os.path.join(self.imgdir, city) # ./images/train/frankfurt/\n",
        "            target_city_dir = os.path.join(self.targetdir, city) # ./gtFine/train/frankfurt/\n",
        "\n",
        "            for img_path in os.listdir(img_city_dir): # frankfurt_000000_000294_leftImg8bit.png\n",
        "                if img_path.endswith(\".png\"):\n",
        "                  self.imgs_path.append(os.path.join(img_city_dir, img_path)) # ./images/train/frankfurt/frankfurt_000000_000294_leftImg8bit.png\n",
        "\n",
        "                  target_color_path = img_path.replace(\"leftImg8bit\", \"gtFine_color\") # frankfurt_000000_000294_gtFine_color.png\n",
        "                  target_labelIds_path = img_path.replace(\"leftImg8bit\", \"gtFine_labelTrainIds\") # frankfurt_000000_000294_gtFine_labelTrainIds.png\n",
        "\n",
        "                  self.targets_color_path.append(os.path.join(target_city_dir, target_color_path)) # ./gtFine/train/frankfurt/frankfurt_000000_000294_gtFine_color.png\n",
        "                  self.targets_labelIds_path.append(os.path.join(target_city_dir, target_labelIds_path)) # ./gtFine/train/frankfurt/frankfurt_000000_000294_gtFine_labelTrainIds.png\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.imgs_path[idx]).convert('RGB')\n",
        "        # image = decode_image(self.imgs_path[idx]).to(dtype=torch.float32)\n",
        "\n",
        "        target_color = Image.open(self.targets_color_path[idx]).convert('RGB')\n",
        "\n",
        "        target_labelIds = cv2.imread(self.targets_labelIds_path[idx], cv2.IMREAD_UNCHANGED).astype(np.long)\n",
        "        # target_labelIds = Image.open(self.targets_labelIds_path[idx])\n",
        "        # target_labelIds =  read_image(self.targets_labelIds_path[idx], mode=ImageReadMode.GRAY)\n",
        "        # target_labelIds = decode_image(self.targets_labelIds_path[idx]).to(dtype=torch.uint8)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "            target_color = self.transform(target_color)\n",
        "        if self.target_transform is not None:\n",
        "            target_labelIds = self.target_transform(target_labelIds)\n",
        "\n",
        "        return image, target_color, target_labelIds\n",
        "        # return image, target_labelIds\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTki3m7aNH10"
      },
      "source": [
        "## GTA5 implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9PeGPXYNKvT"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "from PIL import Image\n",
        "\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "import torchvision.transforms as TF\n",
        "\n",
        "# TODO: decide other augmentations\n",
        "class GTA5(Dataset):\n",
        "    def __init__(self, rootdir, file_names, imgdir=\"images\", targetdir=\"labels\", augment=False, transform=None, target_transform=None):\n",
        "        super(GTA5, self).__init__()\n",
        "\n",
        "        self.rootdir = rootdir\n",
        "\n",
        "        self.targetdir = os.path.join(self.rootdir, targetdir) # ./labels\n",
        "        self.imgdir = os.path.join(self.rootdir, imgdir) # ./images\n",
        "\n",
        "        self.augment = augment\n",
        "\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "        self.imgs_path = []\n",
        "        self.targets_color_path = []\n",
        "        self.targets_labelIds_path = []\n",
        "\n",
        "        for i, image_file in enumerate(file_names): # 00001.png\n",
        "            self.imgs_path.append(os.path.join(self.imgdir, image_file)) #./images/00001.png\n",
        "\n",
        "            target_color_path = image_file # 00001.png\n",
        "            target_labelsId_path = image_file.split(\".\")[0]+\"_labelIds.png\" # 00001_labelIds.png\n",
        "\n",
        "            self.targets_color_path.append(os.path.join(self.targetdir, target_color_path)) #./labels/00001.png\n",
        "            self.targets_labelIds_path.append(os.path.join(self.targetdir, target_labelsId_path)) #./labels/00001_labelIDs.png\n",
        "\n",
        "    def create_target_img(self):\n",
        "        list_ = GTA5Labels_TaskCV2017().list_\n",
        "\n",
        "        for i, img_path in tqdm(enumerate(self.targets_color_path)):\n",
        "            image_numpy = np.asarray(Image.open(img_path).convert('RGB'))\n",
        "\n",
        "            H, W, _ = image_numpy.shape\n",
        "            label_image = 255*np.ones((H, W), dtype=np.uint8)\n",
        "\n",
        "            for label in list_:\n",
        "                label_image[(image_numpy == label.color).all(axis=-1)] = label.ID\n",
        "\n",
        "            new_img = Image.fromarray(label_image)\n",
        "            new_img.save(self.targets_labelIds_path[i])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.imgs_path[idx]).convert('RGB')\n",
        "\n",
        "        target_color = Image.open(self.targets_color_path[idx]).convert('RGB')\n",
        "        target_labelIds = cv2.imread(self.targets_labelIds_path[idx], cv2.IMREAD_UNCHANGED).astype(np.int64)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "            target_color = self.transform(target_color)\n",
        "        if self.target_transform is not None:\n",
        "            target_labelIds = self.target_transform(target_labelIds)\n",
        "\n",
        "        if self.augment:\n",
        "            image, target_color, target_labelIds = self.augment_data(image, target_color, target_labelIds)\n",
        "        return image, target_color, target_labelIds\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs_path)\n",
        "\n",
        "    def augment_data(self, image, target_color, target_labelIds):\n",
        "        # original = image.detach().clone()\n",
        "        val = random.random()\n",
        "    # Geometric Transformations\n",
        "\n",
        "        # Random horizontal flipping\n",
        "        if val < 0.5:\n",
        "            image = TF.functional.hflip(image)\n",
        "            target_color = TF.functional.hflip(target_color)\n",
        "            target_labelIds = TF.functional.hflip(target_labelIds)\n",
        "\n",
        "    # Photometric Transformations\n",
        "\n",
        "        # Gaussian Blur\n",
        "        if val < 0.5:\n",
        "            image = TF.functional.gaussian_blur(image, 5)\n",
        "\n",
        "        # Multiply\n",
        "        if val < 0.5:\n",
        "            factor = random.uniform(0.6, 1.2)\n",
        "            image *= factor\n",
        "            image = torch.clamp(image, 0, 255)\n",
        "\n",
        "        return image, target_color, target_labelIds\n",
        "\n",
        "\n",
        "def GTA5_dataset_splitter(rootdir, train_split_percent, split_seed = None, imgdir=\"images\", targetdir=\"labels\", augment=False, transform=None, target_transform=None):\n",
        "    assert 0.0 < train_split_percent < 1.0, \"train_split_percent should be a float between 0 and 1\"\n",
        "\n",
        "    target_path = os.path.join(rootdir, targetdir) # ./labels\n",
        "    img_path = os.path.join(rootdir, imgdir) # ./images\n",
        "\n",
        "    file_names = [\n",
        "        f for f in os.listdir(img_path)\n",
        "        if f.endswith(\".png\") and os.path.exists(os.path.join(target_path, f.split(\".\")[0]+\"_labelIds.png\"))\n",
        "    ]\n",
        "\n",
        "    if split_seed is not None:\n",
        "        random.seed(split_seed)\n",
        "    random.shuffle(file_names)\n",
        "    random.seed()\n",
        "\n",
        "    split_idx = int(len(file_names) * train_split_percent)\n",
        "\n",
        "    train_files = file_names[:split_idx]\n",
        "    val_files = file_names[split_idx:]\n",
        "\n",
        "    return GTA5(rootdir, train_files, imgdir, targetdir, augment, transform, target_transform), \\\n",
        "           GTA5(rootdir, val_files, imgdir, targetdir, False, transform, target_transform)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2ybDTYzv0ZS"
      },
      "source": [
        "## Bisenet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MbAqrak4u5Ce"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')\n",
        "\n",
        "import torch\n",
        "from torchvision import models\n",
        "\n",
        "class resnet18(torch.nn.Module):\n",
        "    def __init__(self, pretrained=True):\n",
        "        super().__init__()\n",
        "        self.features = models.resnet18(pretrained=pretrained)\n",
        "        self.conv1 = self.features.conv1\n",
        "        self.bn1 = self.features.bn1\n",
        "        self.relu = self.features.relu\n",
        "        self.maxpool1 = self.features.maxpool\n",
        "        self.layer1 = self.features.layer1\n",
        "        self.layer2 = self.features.layer2\n",
        "        self.layer3 = self.features.layer3\n",
        "        self.layer4 = self.features.layer4\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = self.conv1(input)\n",
        "        x = self.relu(self.bn1(x))\n",
        "        x = self.maxpool1(x)\n",
        "        feature1 = self.layer1(x)  # 1 / 4\n",
        "        feature2 = self.layer2(feature1)  # 1 / 8\n",
        "        feature3 = self.layer3(feature2)  # 1 / 16\n",
        "        feature4 = self.layer4(feature3)  # 1 / 32\n",
        "        # global average pooling to build tail\n",
        "        tail = torch.mean(feature4, 3, keepdim=True)\n",
        "        tail = torch.mean(tail, 2, keepdim=True)\n",
        "        return feature3, feature4, tail\n",
        "\n",
        "\n",
        "class resnet101(torch.nn.Module):\n",
        "    def __init__(self, pretrained=True):\n",
        "        super().__init__()\n",
        "        self.features = models.resnet101(pretrained=pretrained)\n",
        "        self.conv1 = self.features.conv1\n",
        "        self.bn1 = self.features.bn1\n",
        "        self.relu = self.features.relu\n",
        "        self.maxpool1 = self.features.maxpool\n",
        "        self.layer1 = self.features.layer1\n",
        "        self.layer2 = self.features.layer2\n",
        "        self.layer3 = self.features.layer3\n",
        "        self.layer4 = self.features.layer4\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = self.conv1(input)\n",
        "        x = self.relu(self.bn1(x))\n",
        "        x = self.maxpool1(x)\n",
        "        feature1 = self.layer1(x)  # 1 / 4\n",
        "        feature2 = self.layer2(feature1)  # 1 / 8\n",
        "        feature3 = self.layer3(feature2)  # 1 / 16\n",
        "        feature4 = self.layer4(feature3)  # 1 / 32\n",
        "        # global average pooling to build tail\n",
        "        tail = torch.mean(feature4, 3, keepdim=True)\n",
        "        tail = torch.mean(tail, 2, keepdim=True)\n",
        "        return feature3, feature4, tail\n",
        "\n",
        "\n",
        "def build_contextpath(name):\n",
        "    model = {\n",
        "        'resnet18': resnet18(pretrained=True),\n",
        "        'resnet101': resnet101(pretrained=True)\n",
        "    }\n",
        "    return model[name]\n",
        "\n",
        "class ConvBlock(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=2, padding=1):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size,\n",
        "                               stride=stride, padding=padding, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = self.conv1(input)\n",
        "        return self.relu(self.bn(x))\n",
        "\n",
        "\n",
        "class Spatial_path(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.convblock1 = ConvBlock(in_channels=3, out_channels=64)\n",
        "        self.convblock2 = ConvBlock(in_channels=64, out_channels=128)\n",
        "        self.convblock3 = ConvBlock(in_channels=128, out_channels=256)\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = self.convblock1(input)\n",
        "        x = self.convblock2(x)\n",
        "        x = self.convblock3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class AttentionRefinementModule(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.in_channels = in_channels\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
        "\n",
        "    def forward(self, input):\n",
        "        # global average pooling\n",
        "        x = self.avgpool(input)\n",
        "        assert self.in_channels == x.size(1), 'in_channels and out_channels should all be {}'.format(x.size(1))\n",
        "        x = self.conv(x)\n",
        "        x = self.sigmoid(self.bn(x))\n",
        "        # x = self.sigmoid(x)\n",
        "        # channels of input and x should be same\n",
        "        x = torch.mul(input, x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class FeatureFusionModule(torch.nn.Module):\n",
        "    def __init__(self, num_classes, in_channels):\n",
        "        super().__init__()\n",
        "        # self.in_channels = input_1.channels + input_2.channels\n",
        "        # resnet101 3328 = 256(from spatial path) + 1024(from context path) + 2048(from context path)\n",
        "        # resnet18  1024 = 256(from spatial path) + 256(from context path) + 512(from context path)\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        self.convblock = ConvBlock(in_channels=self.in_channels, out_channels=num_classes, stride=1)\n",
        "        self.conv1 = nn.Conv2d(num_classes, num_classes, kernel_size=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.conv2 = nn.Conv2d(num_classes, num_classes, kernel_size=1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
        "\n",
        "    def forward(self, input_1, input_2):\n",
        "        x = torch.cat((input_1, input_2), dim=1)\n",
        "        assert self.in_channels == x.size(1), 'in_channels of ConvBlock should be {}'.format(x.size(1))\n",
        "        feature = self.convblock(x)\n",
        "        x = self.avgpool(feature)\n",
        "\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.sigmoid(self.conv2(x))\n",
        "        x = torch.mul(feature, x)\n",
        "        x = torch.add(x, feature)\n",
        "        return x\n",
        "\n",
        "\n",
        "class BiSeNet(torch.nn.Module):\n",
        "    def __init__(self, num_classes, context_path):\n",
        "        super().__init__()\n",
        "        # build spatial path\n",
        "        self.saptial_path = Spatial_path()\n",
        "\n",
        "        # build context path\n",
        "        self.context_path = build_contextpath(name=context_path)\n",
        "\n",
        "        # build attention refinement module  for resnet 101\n",
        "        if context_path == 'resnet101':\n",
        "            self.attention_refinement_module1 = AttentionRefinementModule(1024, 1024)\n",
        "            self.attention_refinement_module2 = AttentionRefinementModule(2048, 2048)\n",
        "            # supervision block\n",
        "            self.supervision1 = nn.Conv2d(in_channels=1024, out_channels=num_classes, kernel_size=1)\n",
        "            self.supervision2 = nn.Conv2d(in_channels=2048, out_channels=num_classes, kernel_size=1)\n",
        "            # build feature fusion module\n",
        "            self.feature_fusion_module = FeatureFusionModule(num_classes, 3328)\n",
        "\n",
        "        elif context_path == 'resnet18':\n",
        "            # build attention refinement module  for resnet 18\n",
        "            self.attention_refinement_module1 = AttentionRefinementModule(256, 256)\n",
        "            self.attention_refinement_module2 = AttentionRefinementModule(512, 512)\n",
        "            # supervision block\n",
        "            self.supervision1 = nn.Conv2d(in_channels=256, out_channels=num_classes, kernel_size=1)\n",
        "            self.supervision2 = nn.Conv2d(in_channels=512, out_channels=num_classes, kernel_size=1)\n",
        "            # build feature fusion module\n",
        "            self.feature_fusion_module = FeatureFusionModule(num_classes, 1024)\n",
        "        else:\n",
        "            print('Error: unspport context_path network \\n')\n",
        "\n",
        "        # build final convolution\n",
        "        self.conv = nn.Conv2d(in_channels=num_classes, out_channels=num_classes, kernel_size=1)\n",
        "\n",
        "        self.init_weight()\n",
        "\n",
        "        self.mul_lr = []\n",
        "        self.mul_lr.append(self.saptial_path)\n",
        "        self.mul_lr.append(self.attention_refinement_module1)\n",
        "        self.mul_lr.append(self.attention_refinement_module2)\n",
        "        self.mul_lr.append(self.supervision1)\n",
        "        self.mul_lr.append(self.supervision2)\n",
        "        self.mul_lr.append(self.feature_fusion_module)\n",
        "        self.mul_lr.append(self.conv)\n",
        "\n",
        "    def init_weight(self):\n",
        "        for name, m in self.named_modules():\n",
        "            if 'context_path' not in name:\n",
        "                if isinstance(m, nn.Conv2d):\n",
        "                    nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
        "                elif isinstance(m, nn.BatchNorm2d):\n",
        "                    m.eps = 1e-5\n",
        "                    m.momentum = 0.1\n",
        "                    nn.init.constant_(m.weight, 1)\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, input):\n",
        "        # output of spatial path\n",
        "        sx = self.saptial_path(input)\n",
        "\n",
        "        # output of context path\n",
        "        cx1, cx2, tail = self.context_path(input)\n",
        "        cx1 = self.attention_refinement_module1(cx1)\n",
        "        cx2 = self.attention_refinement_module2(cx2)\n",
        "        cx2 = torch.mul(cx2, tail)\n",
        "        # upsampling\n",
        "        cx1 = torch.nn.functional.interpolate(cx1, size=sx.size()[-2:], mode='bilinear')\n",
        "        cx2 = torch.nn.functional.interpolate(cx2, size=sx.size()[-2:], mode='bilinear')\n",
        "        cx = torch.cat((cx1, cx2), dim=1)\n",
        "\n",
        "        if self.training == True:\n",
        "            cx1_sup = self.supervision1(cx1)\n",
        "            cx2_sup = self.supervision2(cx2)\n",
        "            cx1_sup = torch.nn.functional.interpolate(cx1_sup, size=input.size()[-2:], mode='bilinear')\n",
        "            cx2_sup = torch.nn.functional.interpolate(cx2_sup, size=input.size()[-2:], mode='bilinear')\n",
        "\n",
        "        # output of feature fusion module\n",
        "        result = self.feature_fusion_module(sx, cx)\n",
        "\n",
        "        # upsampling\n",
        "        result = torch.nn.functional.interpolate(result, scale_factor=8, mode='bilinear')\n",
        "        result = self.conv(result)\n",
        "\n",
        "        if self.training == True:\n",
        "            return result, cx1_sup, cx2_sup\n",
        "\n",
        "        return result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_isSfs1sgJGE"
      },
      "source": [
        "## Wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PLkzB48ZQqC"
      },
      "outputs": [],
      "source": [
        "!pip -q install wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9ffq97eM8Rg"
      },
      "source": [
        "# Train/Val loops"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0KuuGIWNAbF"
      },
      "source": [
        "### Train Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-slUs8vsNAEe"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import wandb\n",
        "\n",
        "# TODO: maybe last class (void) converted to n_classes-1 due to argmax\n",
        "# TODO: waiting confirmation for logging 1 batch every X\n",
        "def train(model:nn.Module, train_loader:DataLoader, criterion:nn.Module, optimizer:optim.Optimizer) -> tuple[float, float, np.ndarray]:\n",
        "    global device\n",
        "    global n_classes\n",
        "    global ENABLE_PRINT\n",
        "    global ENABLE_WANDB_LOG\n",
        "    global train_step\n",
        "    global log_per_epoch\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    num_batch = len(train_loader)\n",
        "    chunk_batch = num_batch//log_per_epoch+1\n",
        "\n",
        "    num_sample = len(train_loader.dataset)\n",
        "    seen_sample = 0\n",
        "\n",
        "    train_loss = 0.0\n",
        "    train_hist = torch.zeros((n_classes,n_classes)).to(device)\n",
        "\n",
        "    for batch_idx, (inputs, _, targets) in enumerate(train_loader):\n",
        "        batch_size = inputs.size(0)\n",
        "        seen_sample += batch_size\n",
        "\n",
        "        inputs, targets = inputs.to(device), targets.squeeze().to(device)\n",
        "\n",
        "        outputs, cx1_sup, cx2_sup = model(inputs)\n",
        "\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss += criterion(cx1_sup, targets)\n",
        "        loss += criterion(cx2_sup, targets)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        predicted = outputs.argmax(1)\n",
        "\n",
        "        hist_batch = torch.zeros((n_classes, n_classes)).to(device)\n",
        "\n",
        "        for i in range(len(inputs)):\n",
        "            hist_batch += fast_hist_torch_cuda(targets[i].detach(), predicted[i].detach(), n_classes)\n",
        "\n",
        "        train_loss += loss.item() * batch_size\n",
        "        train_hist += hist_batch\n",
        "\n",
        "        # TODO: maybe batch_idx+1? same for validate\n",
        "        if ((batch_idx+1) % chunk_batch) == 0:\n",
        "            iou_batch = per_class_iou_cuda(hist_batch)\n",
        "            if ENABLE_PRINT:\n",
        "                    print(f'Training [{seen_sample}/{num_sample} ({100. * seen_sample / num_sample:.0f}%)]')\n",
        "                    print(f'\\tLoss: {loss.item():.6f}')\n",
        "                    print(f\"\\tmIoU: {100.*iou_batch[iou_batch > 0].mean():.4f}\")\n",
        "\n",
        "            if ENABLE_WANDB_LOG:\n",
        "                wandb.log({\n",
        "                        \"train/step\": train_step,\n",
        "                        \"train/batch_loss\": loss.item(),\n",
        "                        \"train/batch_mIou\": 100.*iou_batch[iou_batch > 0].mean()\n",
        "                    },\n",
        "                    commit=True,\n",
        "                )\n",
        "                train_step += 1\n",
        "\n",
        "    train_loss = train_loss / seen_sample\n",
        "\n",
        "    train_iou_class = per_class_iou_cuda(train_hist)\n",
        "    train_mIou = train_iou_class[train_iou_class > 0].mean()\n",
        "\n",
        "    return train_loss, train_mIou, train_hist, train_iou_class\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ON-XdXCkNCrn"
      },
      "source": [
        "### Validation loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13a6MV00QIot"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import wandb\n",
        "\n",
        "def validate(model:nn.Module, val_loader:DataLoader, criterion:nn.Module) -> tuple[float, float, np.ndarray]:\n",
        "    global device\n",
        "    global n_classes\n",
        "    global ENABLE_PRINT\n",
        "    global ENABLE_WANDB_LOG\n",
        "    global val_step\n",
        "    global log_per_epoch\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    num_batch = len(val_loader)\n",
        "    chunk_batch = num_batch//log_per_epoch+1\n",
        "\n",
        "    num_sample = len(val_loader.dataset)\n",
        "    seen_sample = 0\n",
        "    chunk_sample = 0\n",
        "\n",
        "    val_loss = 0.0\n",
        "    val_hist = torch.zeros((n_classes,n_classes)).to(device)\n",
        "\n",
        "    chunk_loss = 0.0\n",
        "    chunk_hist = torch.zeros((n_classes,n_classes)).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, _, targets) in enumerate(val_loader):\n",
        "            batch_size = inputs.size(0)\n",
        "\n",
        "            inputs, targets = inputs.to(device), targets.squeeze().to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            predicted = outputs.argmax(1)\n",
        "\n",
        "            hist_batch = torch.zeros((n_classes, n_classes)).to(device)\n",
        "            for i in range(len(inputs)):\n",
        "                hist_batch += fast_hist_torch_cuda(targets[i].detach(), predicted[i].detach(), n_classes)\n",
        "\n",
        "            chunk_sample += batch_size\n",
        "            chunk_loss += loss.item() * batch_size\n",
        "            chunk_hist += hist_batch\n",
        "\n",
        "            if ((batch_idx+1) % chunk_batch) == 0:\n",
        "                seen_sample += chunk_sample\n",
        "                val_loss += chunk_loss\n",
        "                val_hist += chunk_hist\n",
        "\n",
        "                if ENABLE_PRINT:\n",
        "                    iou_batch = per_class_iou_cuda(hist_batch)\n",
        "                    print(f'Validation [{seen_sample}/{num_sample} ({100. * seen_sample / num_sample:.0f}%)]')\n",
        "                    print(f'\\tLoss: {loss.item():.6f}')\n",
        "                    print(f\"\\tmIoU: {100.*iou_batch[iou_batch > 0].mean():.4f}\")\n",
        "\n",
        "                if ENABLE_WANDB_LOG:\n",
        "                    iou_batch = per_class_iou_cuda(chunk_hist)\n",
        "                    wandb.log({\n",
        "                            \"validate/step\": val_step,\n",
        "                            \"validate/batch_loss\": chunk_loss/chunk_sample,\n",
        "                            \"validate/batch_mIou\": 100.*iou_batch[iou_batch > 0].mean()\n",
        "                        },\n",
        "                        commit=True,\n",
        "                    )\n",
        "\n",
        "                    val_step += 1\n",
        "\n",
        "                chunk_sample = 0\n",
        "                chunk_loss = 0.0\n",
        "                chunk_hist = torch.zeros((n_classes, n_classes)).to(device)\n",
        "\n",
        "        if chunk_sample > 0:\n",
        "            seen_sample += chunk_sample\n",
        "            val_loss += chunk_loss\n",
        "            val_hist += chunk_hist\n",
        "\n",
        "            if ENABLE_PRINT:\n",
        "                iou_batch = per_class_iou_cuda(hist_batch)\n",
        "                print(f'Validation [{seen_sample}/{num_sample} ({100. * seen_sample / num_sample:.0f}%)]')\n",
        "                print(f'\\tLoss: {loss.item():.6f}')\n",
        "                print(f\"\\tmIoU: {100.*iou_batch[iou_batch > 0].mean():.4f}\")\n",
        "\n",
        "            if ENABLE_WANDB_LOG:\n",
        "                iou_batch = per_class_iou_cuda(chunk_hist)\n",
        "                wandb.log({\n",
        "                        \"validate/step\": val_step,\n",
        "                        \"validate/batch_loss\": chunk_loss/chunk_sample,\n",
        "                        \"validate/batch_mIou\": 100.*iou_batch[iou_batch > 0].mean()\n",
        "                    },\n",
        "                    commit=True,\n",
        "                )\n",
        "\n",
        "                val_step += 1\n",
        "\n",
        "    val_loss = val_loss / seen_sample\n",
        "\n",
        "    val_iou_class = per_class_iou_cuda(val_hist)\n",
        "    val_mIou = val_iou_class[val_iou_class > 0].mean()\n",
        "\n",
        "    return val_loss, val_mIou, val_hist, val_iou_class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgPi7OfDNLJX"
      },
      "source": [
        "# Machine learning"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: something not working with validate/batch_miou when epoch starts, always a peak\n",
        "# Possibly since less images -> less classes seen -> lower denominator when calculating mIou\n",
        "# TODO: something wrong with validate, batch_loss going up and batch_miou going down while epoch-level metrics are fine\n",
        "# TODO: num of log from validate and train is different?\n",
        "# Found validate/step went back to 0 -> typo: validate_step instead of val_step\n",
        "\n",
        "def pipeline():\n",
        "    from torch.utils.data import DataLoader\n",
        "    import torchvision.transforms as TF\n",
        "    import torch.nn as nn\n",
        "    import torch.optim as optim\n",
        "    import wandb\n",
        "    import os\n",
        "\n",
        "    global device\n",
        "    global n_classes\n",
        "    global ENABLE_PRINT\n",
        "    global ENABLE_WANDB_LOG\n",
        "    global train_step\n",
        "    global val_step\n",
        "    global log_per_epoch\n",
        "\n",
        "    ENABLE_PRINT = False\n",
        "    ENABLE_WANDB_LOG = True\n",
        "    train_step = 0\n",
        "    val_step = 0\n",
        "    log_per_epoch = 20\n",
        "\n",
        "    models_root_dir = \"./models\"\n",
        "    !rm -rf {models_root_dir}\n",
        "    !mkdir {models_root_dir}\n",
        "\n",
        "    B = 3\n",
        "    H = 512\n",
        "    W = 1024\n",
        "    n_classes = 19\n",
        "\n",
        "    backbone = \"BiSeNet\"\n",
        "    context_path = \"resnet18\"\n",
        "\n",
        "    # FIXME: not work, look under where run config is defined\n",
        "    start_epoch = 0 # <--------- Last epoch that i completed (in this script i will perform from the start+1 to end)\n",
        "    end_epoch = 2\n",
        "    max_epoch = 50\n",
        "\n",
        "    assert start_epoch < end_epoch <= max_epoch, \"Check your start/end/max epoch settings.\"\n",
        "\n",
        "    init_lr=2.5e-3\n",
        "    lr_decay_iter = 1\n",
        "    momentum=0.9\n",
        "    weight_decay=1e-4\n",
        "    dataset = \"Cityscapes\"\n",
        "\n",
        "    transform = TF.Compose([\n",
        "        TF.ToTensor(),\n",
        "        TF.Resize((H,W)),\n",
        "        TF.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                    std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    target_transform = TF.Compose([\n",
        "        TF.ToTensor(),\n",
        "        TF.Resize((H, W), interpolation=TF.InterpolationMode.NEAREST),\n",
        "    ])\n",
        "\n",
        "    # Dataset objects\n",
        "    if dataset == \"Cityscapes\":\n",
        "        data_train = CityScapes(\"./Cityscapes/Cityspaces\", split=\"train\", transform=transform, target_transform=target_transform)\n",
        "        data_val = CityScapes(\"./Cityscapes/Cityspaces\", split=\"val\", transform=transform, target_transform=target_transform)\n",
        "    elif dataset == \"GTA5\":\n",
        "        data_train, data_val = GTA5_dataset_splitter(\"./Gta5_extended\", train_split_percent=0.8, split_seed=42, transform=transform, target_transform=target_transform)\n",
        "    else:\n",
        "        raise Exception(\"Wrong dataset name\")\n",
        "    train_loader = DataLoader(data_train, batch_size=B, shuffle=True)\n",
        "    val_loader = DataLoader(data_val, batch_size=B, shuffle=True)\n",
        "\n",
        "    # Architecture\n",
        "    if backbone == \"BiSeNet\":\n",
        "        model = BiSeNet(n_classes, context_path).to(device)\n",
        "        architecture = backbone+\"-\"+context_path\n",
        "    elif backbone == \"DeepLab\":\n",
        "        model = get_deeplab_v2(num_classes=n_classes, pretrain=True).to(device)\n",
        "        architecture = backbone\n",
        "    else:\n",
        "        raise Exception(\"Wrong model name\")\n",
        "\n",
        "    # The other 2\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=255)\n",
        "    optimizer = optim.SGD(model.parameters(), lr=init_lr, momentum=momentum, weight_decay=weight_decay)\n",
        "\n",
        "    # TODO: wandb can't let us reuse the same run_id, need way to manage it\n",
        "    # Wandb setup and metrics\n",
        "    run_name = f\"step_2B_new\"\n",
        "    run_id = f\"{run_name}_{architecture}_{dataset}\"\n",
        "    run = wandb.init(\n",
        "        entity=\"Machine_learning_and_Deep_learning_labs\",\n",
        "        project=\"Semantic Segmentation\",\n",
        "        name=run_name,\n",
        "        id=run_id,\n",
        "        resume=\"allow\", # <----------------  IMPORTANT CONFIG KEY\n",
        "        config={\n",
        "            \"initial_learning_rate\": init_lr,\n",
        "            \"lr_decay_iter\": lr_decay_iter,\n",
        "            \"momentum\": momentum,\n",
        "            \"weight_decay\": weight_decay,\n",
        "            \"architecture\": architecture,\n",
        "            \"dataset\": dataset,\n",
        "            \"start_epoch\": start_epoch,\n",
        "            \"end_epoch\": end_epoch,\n",
        "            \"max_epoch\": max_epoch,\n",
        "            \"batch\": B,\n",
        "            \"lr_scheduler\": \"poly\"\n",
        "        },\n",
        "    )\n",
        "\n",
        "    wandb.define_metric(\"epoch/step\")\n",
        "    wandb.define_metric(\"epoch/*\", step_metric=\"epoch/step\")\n",
        "\n",
        "    wandb.define_metric(\"train/step\")\n",
        "    wandb.define_metric(\"train/*\", step_metric=\"train/step\")\n",
        "\n",
        "    wandb.define_metric(\"validate/step\")\n",
        "    wandb.define_metric(\"validate/*\", step_metric=\"validate/step\")\n",
        "\n",
        "    # FIXME: does not work, problems with run id\n",
        "    # Loading form a starting point\n",
        "    if start_epoch > 0:\n",
        "        artifact = run.use_artifact(f'Machine_learning_and_Deep_learning_labs/Semantic Segmentation/{run_id}:epoch_{start_epoch}', type='model')\n",
        "        artifact_dir = artifact.download()\n",
        "\n",
        "        artifact_path = os.path.join(artifact_dir, run_id+f\"_epoch_{start_epoch}.pth\")\n",
        "\n",
        "        checkpoint = torch.load(artifact_path, map_location=device)\n",
        "\n",
        "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
        "\n",
        "        train_step = checkpoint[\"train_step\"]+1\n",
        "        val_step = checkpoint[\"validate_step\"]+1\n",
        "\n",
        "    # Main Loop\n",
        "    for epoch in range(start_epoch+1, end_epoch+1):\n",
        "        print(\"-----------------------------\")\n",
        "        print(f\"Epoch {epoch}\")\n",
        "\n",
        "        lr = poly_lr_scheduler(optimizer, init_lr, epoch-1, max_iter=max_epoch)\n",
        "\n",
        "        print(f\"[Poly LR] 100xLR: {100.*lr:.6f}\")\n",
        "\n",
        "        run.log({\n",
        "            \"epoch/step\": epoch,\n",
        "            \"epoch/100xlearning_rate\": 100.*lr,\n",
        "        })\n",
        "\n",
        "        train_loss, train_mIou, train_hist, train_mIou_class = train(model, train_loader, criterion, optimizer)\n",
        "\n",
        "        print(f'[Train Loss] : {train_loss:.6f} [mIoU]: {100.*train_mIou:.2f}%')\n",
        "\n",
        "        # log_confusion_matrix(\"Confusion Matrix - Train\", train_hist, \"epoch/train_confusion_matrix\", \"epoch/step\", epoch)\n",
        "        run.log({\n",
        "                \"epoch/step\": epoch,\n",
        "                \"epoch/train_loss\": train_loss,\n",
        "                \"epoch/train_mIou\": 100*train_mIou\n",
        "            },\n",
        "            commit=True,\n",
        "        )\n",
        "\n",
        "        val_loss, val_mIou, val_hist, val_mIou_class = validate(model, val_loader, criterion)\n",
        "\n",
        "        print(f'[Validation Loss] : {val_loss:.6f} [mIoU]: {100.*val_mIou:.2f}%')\n",
        "\n",
        "        # log_confusion_matrix(\"Confusion Matrix - Validate\", val_hist, \"epoch/validate_confusion_matrix\", \"epoch/step\", epoch)\n",
        "        run.log({\n",
        "                \"epoch/step\": epoch,\n",
        "                \"epoch/val_loss\": val_loss,\n",
        "                \"epoch/val_mIou\": 100*val_mIou\n",
        "            },\n",
        "            commit=True\n",
        "        )\n",
        "\n",
        "\n",
        "        if epoch % 2 == 0 or epoch == end_epoch:\n",
        "            checkpoint = {\n",
        "                \"model_state_dict\": model.state_dict(),\n",
        "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "                \"train_step\": train_step,\n",
        "                \"validate_step\": val_step,\n",
        "            }\n",
        "\n",
        "            file_name = f\"{run_id}_epoch_{epoch}.pth\"\n",
        "\n",
        "            # TODO: add some tables to artifact to enable comparisons\n",
        "\n",
        "            # Saving the progress\n",
        "            file_path = os.path.join(models_root_dir, file_name)\n",
        "            torch.save(checkpoint, file_path)\n",
        "\n",
        "            print(f\"Model saved to {file_path}\")\n",
        "\n",
        "            artifact = wandb.Artifact(name=run_id, type=\"model\")\n",
        "            artifact.add_file(file_path)\n",
        "\n",
        "            run.log_artifact(artifact, aliases=[\"latest\", f\"epoch_{epoch}\"])\n",
        "\n",
        "        if (epoch % 10) == 0:\n",
        "            log_confusion_matrix_cuda(\"Confusion Matrix - Train\", train_hist, \"epoch/train_confusion_matrix\", \"epoch/step\", epoch)\n",
        "            log_confusion_matrix_cuda(\"Confusion Matrix - Validate\", val_hist, \"epoch/validate_confusion_matrix\", \"epoch/step\", epoch)\n",
        "\n",
        "            log_bar_chart_ioU(f\"Train IoU per class - epoch {epoch}\", [c.name for c in GTA5Labels_TaskCV2017().list_ if c.name != \"void\"], train_mIou, train_mIou_class, \"epoch/train_Iou_class\", \"epoch/step\", epoch)\n",
        "            log_bar_chart_ioU(f\"Validate IoU per class - epoch {epoch}\", [c.name for c in GTA5Labels_TaskCV2017().list_ if c.name != \"void\"], val_mIou, val_mIou_class, \"epoch/validate_Iou_class\", \"epoch/step\", epoch)\n",
        "\n",
        "\n",
        "    # TODO: need to check if works\n",
        "    # TODO: need to check if works\n",
        "    run.config[\"end_epoch\"] = end_epoch\n",
        "    run.config[\"start_epoch\"] = start_epoch\n",
        "\n",
        "    if end_epoch == max_epoch:\n",
        "        # TODO: fix types, from list and strings to numbers\n",
        "        # TODO: add model param\n",
        "\n",
        "        mean_latency, std_latency, mean_fps = latency(device, model, H=512, W=1024)\n",
        "\n",
        "\n",
        "        run.log({\n",
        "            \"model/flops\": num_flops(device, model, 512, 1024),\n",
        "            \"model/latency_mean\": mean_latency,\n",
        "            \"model/latency_std\": std_latency,\n",
        "            \"model/mean_fps\": mean_fps,\n",
        "            \"model/param\": num_param(model)\n",
        "        })\n",
        "\n",
        "    run.finish()\n",
        "\n",
        "wandb.finish()\n",
        "pipeline()\n"
      ],
      "metadata": {
        "id": "QH9ZSG-m617c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6e2b7a56-d66f-40ab-f0f2-aca97151c05f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/100xlearning_rate</td><td>▁</td></tr><tr><td>epoch/step</td><td>▁▁</td></tr><tr><td>epoch/train_loss</td><td>▁</td></tr><tr><td>epoch/train_mIou</td><td>▁</td></tr><tr><td>train/batch_loss</td><td>██▅▄▄▅▃▃▄▂▃▃▂▄▁▂▂▂▃</td></tr><tr><td>train/batch_mIou</td><td>▁▃▆▅▅▄▇▂▆▆▅▃▅▄▄█▆▅▃</td></tr><tr><td>train/step</td><td>▁▁▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇██</td></tr><tr><td>validate/batch_loss</td><td>▁</td></tr><tr><td>validate/batch_mIou</td><td>▁</td></tr><tr><td>validate/step</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/100xlearning_rate</td><td>0.25</td></tr><tr><td>epoch/step</td><td>1</td></tr><tr><td>epoch/train_loss</td><td>2.48955</td></tr><tr><td>epoch/train_mIou</td><td>26.33832</td></tr><tr><td>train/batch_loss</td><td>2.01681</td></tr><tr><td>train/batch_mIou</td><td>48.04531</td></tr><tr><td>train/step</td><td>18</td></tr><tr><td>validate/batch_loss</td><td>0.45048</td></tr><tr><td>validate/batch_mIou</td><td>46.92132</td></tr><tr><td>validate/step</td><td>0</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">step_2B_new</strong> at: <a href='https://wandb.ai/Machine_learning_and_Deep_learning_labs/Semantic%20Segmentation/runs/step_2B_new_BiSeNet-resnet18_Cityscapes' target=\"_blank\">https://wandb.ai/Machine_learning_and_Deep_learning_labs/Semantic%20Segmentation/runs/step_2B_new_BiSeNet-resnet18_Cityscapes</a><br> View project at: <a href='https://wandb.ai/Machine_learning_and_Deep_learning_labs/Semantic%20Segmentation' target=\"_blank\">https://wandb.ai/Machine_learning_and_Deep_learning_labs/Semantic%20Segmentation</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250513_155725-step_2B_new_BiSeNet-resnet18_Cityscapes/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250513_160639-step_2B_new_BiSeNet-resnet18_Cityscapes</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Resuming run <strong><a href='https://wandb.ai/Machine_learning_and_Deep_learning_labs/Semantic%20Segmentation/runs/step_2B_new_BiSeNet-resnet18_Cityscapes' target=\"_blank\">step_2B_new</a></strong> to <a href='https://wandb.ai/Machine_learning_and_Deep_learning_labs/Semantic%20Segmentation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/Machine_learning_and_Deep_learning_labs/Semantic%20Segmentation' target=\"_blank\">https://wandb.ai/Machine_learning_and_Deep_learning_labs/Semantic%20Segmentation</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/Machine_learning_and_Deep_learning_labs/Semantic%20Segmentation/runs/step_2B_new_BiSeNet-resnet18_Cityscapes' target=\"_blank\">https://wandb.ai/Machine_learning_and_Deep_learning_labs/Semantic%20Segmentation/runs/step_2B_new_BiSeNet-resnet18_Cityscapes</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------\n",
            "Epoch 1\n",
            "[Poly LR] 100xLR: 0.250000\n",
            "[Train Loss] : 2.538756 [mIoU]: 26.77%\n",
            "[Validation Loss] : 0.479447 [mIoU]: 38.61%\n",
            "-----------------------------\n",
            "Epoch 2\n",
            "[Poly LR] 100xLR: 0.245495\n",
            "[Train Loss] : 1.453502 [mIoU]: 44.45%\n",
            "[Validation Loss] : 0.413842 [mIoU]: 44.99%\n",
            "Model saved to ./models/step_2B_new_BiSeNet-resnet18_Cityscapes_epoch_2.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/100xlearning_rate</td><td>█▁</td></tr><tr><td>epoch/step</td><td>▁▁▁███</td></tr><tr><td>epoch/train_loss</td><td>█▁</td></tr><tr><td>epoch/train_mIou</td><td>▁█</td></tr><tr><td>epoch/val_loss</td><td>█▁</td></tr><tr><td>epoch/val_mIou</td><td>▁█</td></tr><tr><td>train/batch_loss</td><td>█▇▅▄▃▃▃▃▃▃▂▂▂▂▁▃▁▁▁▂▂▂▂▂▁▁▁▁▁▁▁▁▃▂▁▁▁▂</td></tr><tr><td>train/batch_mIou</td><td>▁▂▄▆▇▄▅▅▄▅▆▃▃▆▅▅▆▄▃▅▄▅▅▃▄▅▅▆▃▃▃█▂▃▄▄▇▃</td></tr><tr><td>train/step</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>validate/batch_loss</td><td>▂▄▅▄▄▅▅▆█▄▄▃▄▆▅▆▄▅█▃▁▂▁▁▄▁▃▃▄▄▂▂▂▃▁▁▃▇</td></tr><tr><td>validate/batch_mIou</td><td>▆▄▂▃▁▄▄▃▄▄▂▄▆▄▄▃▄▄▂▂▅▅█▆▄▄▄▆▆▆▄▅▄▅▇██▇</td></tr><tr><td>validate/step</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/100xlearning_rate</td><td>0.2455</td></tr><tr><td>epoch/step</td><td>2</td></tr><tr><td>epoch/train_loss</td><td>1.4535</td></tr><tr><td>epoch/train_mIou</td><td>44.44945</td></tr><tr><td>epoch/val_loss</td><td>0.41384</td></tr><tr><td>epoch/val_mIou</td><td>44.98872</td></tr><tr><td>train/batch_loss</td><td>1.39806</td></tr><tr><td>train/batch_mIou</td><td>48.4876</td></tr><tr><td>train/step</td><td>37</td></tr><tr><td>validate/batch_loss</td><td>0.53728</td></tr><tr><td>validate/batch_mIou</td><td>49.09648</td></tr><tr><td>validate/step</td><td>37</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">step_2B_new</strong> at: <a href='https://wandb.ai/Machine_learning_and_Deep_learning_labs/Semantic%20Segmentation/runs/step_2B_new_BiSeNet-resnet18_Cityscapes' target=\"_blank\">https://wandb.ai/Machine_learning_and_Deep_learning_labs/Semantic%20Segmentation/runs/step_2B_new_BiSeNet-resnet18_Cityscapes</a><br> View project at: <a href='https://wandb.ai/Machine_learning_and_Deep_learning_labs/Semantic%20Segmentation' target=\"_blank\">https://wandb.ai/Machine_learning_and_Deep_learning_labs/Semantic%20Segmentation</a><br>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250513_160639-step_2B_new_BiSeNet-resnet18_Cityscapes/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "iAxaUMhvM4E4",
        "gEx91uPqlFv3",
        "YZME_xiNNE-g",
        "LTki3m7aNH10",
        "O2ybDTYzv0ZS",
        "XPTyvGv0fQd8",
        "_isSfs1sgJGE"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6b05864ee44849cf87e9c44b24b5eeeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e1f088c5729c4a9c91f28438667278e7",
              "IPY_MODEL_173cbbd900da41b1bc87a5da64855951",
              "IPY_MODEL_dbb3b93d9f6e4c17955f5dfe86b3e9bd"
            ],
            "layout": "IPY_MODEL_0d2ae9857a2a4865b4ade50dffe2434f"
          }
        },
        "e1f088c5729c4a9c91f28438667278e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_827ce8d8d695468c851a6dbd8f853ef8",
            "placeholder": "​",
            "style": "IPY_MODEL_304a2517f3d340278caf28fd6ce4444e",
            "value": "Extracting PNGs: 100%"
          }
        },
        "173cbbd900da41b1bc87a5da64855951": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39d61cc6255346a5885d7fd6ffcf3c17",
            "max": 6216,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8a1f980960754f7fa67c3509e597710b",
            "value": 6216
          }
        },
        "dbb3b93d9f6e4c17955f5dfe86b3e9bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6bf5c1f6e984649a3beb0ab2e98891e",
            "placeholder": "​",
            "style": "IPY_MODEL_e1337057ee17487ab79d2302d2150bdc",
            "value": " 6216/6216 [00:43&lt;00:00, 80.76it/s]"
          }
        },
        "0d2ae9857a2a4865b4ade50dffe2434f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "827ce8d8d695468c851a6dbd8f853ef8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "304a2517f3d340278caf28fd6ce4444e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "39d61cc6255346a5885d7fd6ffcf3c17": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a1f980960754f7fa67c3509e597710b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e6bf5c1f6e984649a3beb0ab2e98891e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1337057ee17487ab79d2302d2150bdc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}