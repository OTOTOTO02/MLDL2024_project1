{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CityScapes download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import pretty_extract\n",
    "\n",
    "# !pip install -q gdown\n",
    "\n",
    "# file_id = \"1MI8QsvjW0R6WDJiL49L7sDGpPWYAQB6O\"\n",
    "# !gdown https://drive.google.com/uc?id={file_id}\n",
    "\n",
    "# pretty_extract(\"Cityscapes.zip\", \".\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GTA5 download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import pretty_extract\n",
    "\n",
    "# !pip install -q gdown\n",
    "\n",
    "# file_id = \"1PWavqXDxuifsyYvs2PFua9sdMl0JG8AE\"\n",
    "# !gdown https://drive.google.com/uc?id={file_id}\n",
    "\n",
    "# pretty_extract(\"Gta5_extended.zip\", \"./Gta5_extended\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepLab weights download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q gdown\n",
    "\n",
    "# file_id = \"1KgYgBTmvq7UcBwKui2b4TomnbTmzJMBf\"\n",
    "# !gdown https://drive.google.com/uc?id={file_id}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Utilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as TF\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import tensorToImageCompatible, decode_segmap\n",
    "from datasets import cityscapes, gta5 \n",
    "\n",
    "def test_dataset():\n",
    "    B = 3\n",
    "    H = 512\n",
    "    W = 1024\n",
    "    dataset = \"GTA5\"\n",
    "\n",
    "    transform = TF.Compose([\n",
    "        TF.ToTensor(),\n",
    "        TF.Resize((H,W)),\n",
    "        TF.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                    std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    target_transform = TF.Compose([\n",
    "        TF.ToTensor(),\n",
    "        TF.Resize((H, W), interpolation=TF.InterpolationMode.NEAREST),\n",
    "    ])\n",
    "\n",
    "    if dataset == \"Cityscapes\":\n",
    "        data = cityscapes.CityScapes(\"./Cityscapes/Cityspaces\", split=\"train\", transform=transform, target_transform=target_transform)\n",
    "    elif dataset == \"GTA5\":\n",
    "        data, _ = gta5.GTA5_dataset_splitter(\"./Gta5_extended\", train_split_percent=0.6, split_seed=42, augment=False, transform=transform, target_transform=target_transform)\n",
    "    else:\n",
    "        raise Exception(\"Wrong dataset name\")\n",
    "\n",
    "    dataloader = DataLoader(data, batch_size=B, shuffle=False)\n",
    "    i = 0\n",
    "    img_tensor, color_tensor, label = next(iter(dataloader))\n",
    "\n",
    "    img = tensorToImageCompatible(img_tensor[i])\n",
    "\n",
    "    color = tensorToImageCompatible(color_tensor[i])\n",
    "    decoded_from_labelId = decode_segmap(label[i, 0])\n",
    "\n",
    "    fig, ax = plt.subplots(2,2, figsize=(10,10), layout=\"tight\")\n",
    "\n",
    "    ax[0,0].set_title(\"Image\")\n",
    "    ax[0,0].imshow(img)\n",
    "    ax[0,0].axis('off')\n",
    "\n",
    "    ax[0,1].set_title(\"Colored by label\")\n",
    "    ax[0,1].imshow(color)\n",
    "    ax[0,1].axis('off')\n",
    "\n",
    "    ax[1,0].set_title(\"Reconstructed from class ID\")\n",
    "    ax[1,0].imshow(decoded_from_labelId)\n",
    "    ax[1,0].axis('off')\n",
    "\n",
    "    ax[1,1].set_title(\"Raw Classes\")\n",
    "    ax[1,1].imshow(label[i, 0])\n",
    "    ax[1,1].axis('off')\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "# test_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cityscapes image:\n",
    "\n",
    "![title](cityscapes_example.png)\n",
    "\n",
    "Gta image:\n",
    "\n",
    "![title](gta_example.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENABLE_PRINT = False\n",
    "ENABLE_WANDB_LOG = True\n",
    "log_per_epoch = 20\n",
    "n_classes = 19\n",
    "\n",
    "train_step = 0\n",
    "val_step = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: something not working with validate/batch_miou when epoch starts, always a peak\n",
    "# Possibly since less images -> less classes seen -> lower denominator when calculating mIou\n",
    "# TODO: something wrong with validate, batch_loss going up and batch_miou going down while epoch-level metrics are fine\n",
    "# TODO: num of log from validate and train is different?\n",
    "# Found validate/step went back to 0 -> typo: validate_step instead of val_step\n",
    "\n",
    "def pipeline():\n",
    "    from torch.utils.data import DataLoader\n",
    "    import torchvision.transforms as TF\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    import wandb\n",
    "    import os\n",
    "\n",
    "    from train import train, validate\n",
    "    from utils import poly_lr_scheduler, num_flops, latency, log_confusion_matrix\n",
    "    from datasets import cityscapes, gta5 \n",
    "    from models.bisenet.build_bisenet import BiSeNet\n",
    "    from models.deeplabv2.deeplabv2 import get_deeplab_v2\n",
    "\n",
    "    global device\n",
    "    global n_classes\n",
    "    global ENABLE_PRINT\n",
    "    global ENABLE_WANDB_LOG\n",
    "    global train_step\n",
    "    global val_step\n",
    "    global log_per_epoch\n",
    "\n",
    "    ENABLE_PRINT = False\n",
    "    ENABLE_WANDB_LOG = True\n",
    "    train_step = 0\n",
    "    val_step = 0\n",
    "    log_per_epoch = 20\n",
    "\n",
    "    models_root_dir = \"./models\"\n",
    "    !rm -rf {models_root_dir}\n",
    "    !mkdir {models_root_dir}\n",
    "\n",
    "    B = 3\n",
    "    H = 512\n",
    "    W = 1024\n",
    "    n_classes = 19\n",
    "\n",
    "    backbone = \"BiSeNet\"\n",
    "    context_path = \"resnet101\"\n",
    "\n",
    "    start_epoch = 0\n",
    "    end_epoch = 2\n",
    "    max_epoch = 50\n",
    "\n",
    "    assert start_epoch < end_epoch <= max_epoch, \"Check your start/end/max epoch settings.\"\n",
    "\n",
    "    init_lr=0.001\n",
    "    lr_decay_iter = 1\n",
    "    momentum=0.9\n",
    "    weight_decay=5e-4\n",
    "    dataset = \"Cityscapes\"\n",
    "\n",
    "    transform = TF.Compose([\n",
    "        TF.ToTensor(),\n",
    "        TF.Resize((H,W)),\n",
    "        TF.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                    std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    target_transform = TF.Compose([\n",
    "        TF.ToTensor(),\n",
    "        TF.Resize((H, W), interpolation=TF.InterpolationMode.NEAREST),\n",
    "    ])\n",
    "\n",
    "    # Dataset objects\n",
    "    if dataset == \"Cityscapes\":\n",
    "        data_train = cityscapes.CityScapes(\"./Cityscapes/Cityspaces\", split=\"train\", transform=transform, target_transform=target_transform)\n",
    "        data_val = cityscapes.CityScapes(\"./Cityscapes/Cityspaces\", split=\"val\", transform=transform, target_transform=target_transform)\n",
    "    elif dataset == \"GTA5\":\n",
    "        data_train, data_val = gta5.GTA5_dataset_splitter(\"./Gta5_extended\", train_split_percent=0.8, split_seed=42, transform=transform, target_transform=target_transform)\n",
    "    else:\n",
    "        raise Exception(\"Wrong dataset name\")\n",
    "    train_loader = DataLoader(data_train, batch_size=B, shuffle=True)\n",
    "    val_loader = DataLoader(data_val, batch_size=B, shuffle=True)\n",
    "\n",
    "    # Architecture\n",
    "    if backbone == \"BiSeNet\":\n",
    "        model = BiSeNet(n_classes, context_path).to(device)\n",
    "        architecture = backbone+\"-\"+context_path\n",
    "    elif backbone == \"DeepLab\":\n",
    "        model = get_deeplab_v2(num_classes=n_classes, pretrain=True).to(device)\n",
    "        architecture = backbone\n",
    "    else:\n",
    "        raise Exception(\"Wrong model name\")\n",
    "\n",
    "    # The other 2\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=255)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=init_lr, momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "    # TODO: wandb can't let us reuse the same run_id, need way to manage it\n",
    "    # Wandb setup and metrics\n",
    "    run_name = f\"step_2\"\n",
    "    run_id = f\"{run_name}_{architecture}_{dataset}\"\n",
    "    run = wandb.init(\n",
    "        entity=\"Machine_learning_and_Deep_learning_labs\",\n",
    "        project=\"Semantic Segmentation\",\n",
    "        name=run_name,\n",
    "        resume=\"allow\", # <----------------  IMPORTANT CONFIG KEY\n",
    "        config={\n",
    "            \"initial_learning_rate\": init_lr,\n",
    "            \"lr_decay_iter\": lr_decay_iter,\n",
    "            \"momentum\": momentum,\n",
    "            \"weight_decay\": weight_decay,\n",
    "            \"architecture\": architecture,\n",
    "            \"dataset\": dataset,\n",
    "            \"start_epoch\": start_epoch,\n",
    "            \"end_epoch\": end_epoch,\n",
    "            \"max_epoch\": max_epoch,\n",
    "            \"batch\": B,\n",
    "            \"lr_scheduler\": \"poly\"\n",
    "        },\n",
    "    )\n",
    "\n",
    "    wandb.define_metric(\"epoch/step\")\n",
    "    wandb.define_metric(\"epoch/*\", step_metric=\"epoch/step\")\n",
    "\n",
    "    wandb.define_metric(\"train/step\")\n",
    "    wandb.define_metric(\"train/*\", step_metric=\"train/step\")\n",
    "\n",
    "    wandb.define_metric(\"validate/step\")\n",
    "    wandb.define_metric(\"validate/*\", step_metric=\"validate/step\")\n",
    "\n",
    "    # Loading form a starting point\n",
    "    if start_epoch > 0:\n",
    "        artifact = run.use_artifact(f'Machine_learning_and_Deep_learning_labs/Semantic Segmentation/{run_id}:epoch_{start_epoch}', type='model')\n",
    "        artifact_dir = artifact.download()\n",
    "\n",
    "        artifact_path = os.path.join(artifact_dir, run_id+f\"_epoch_{start_epoch}.pth\")\n",
    "\n",
    "        checkpoint = torch.load(artifact_path, map_location=device)\n",
    "\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "\n",
    "        train_step = checkpoint[\"train_step\"]+1\n",
    "        val_step = checkpoint[\"validate_step\"]+1\n",
    "\n",
    "    # Main Loop\n",
    "    for epoch in range(start_epoch+1, end_epoch+1):\n",
    "        print(\"-----------------------------\")\n",
    "        print(f\"Epoch {epoch}\")\n",
    "\n",
    "        lr = poly_lr_scheduler(optimizer, init_lr, epoch-1, max_iter=max_epoch)\n",
    "\n",
    "        print(f\"[Poly LR] 100xLR: {100.*lr:.6f}\")\n",
    "\n",
    "        run.log({\n",
    "            \"epoch/step\": epoch,\n",
    "            \"epoch/100xlearning_rate\": 100.*lr,\n",
    "        })\n",
    "\n",
    "        train_loss, train_mIou, train_hist = train(model, train_loader, criterion, optimizer)\n",
    "\n",
    "        print(f'[Train Loss] : {train_loss:.6f} [mIoU]: {100.*train_mIou:.2f}%')\n",
    "\n",
    "        # log_confusion_matrix(\"Confusion Matrix - Train\", train_hist, \"epoch/train_confusion_matrix\", \"epoch/step\", epoch)\n",
    "        run.log({\n",
    "                \"epoch/step\": epoch,\n",
    "                \"epoch/train_loss\": train_loss,\n",
    "                \"epoch/train_mIou\": 100*train_mIou\n",
    "            },\n",
    "            commit=True,\n",
    "        )\n",
    "\n",
    "        val_loss, val_mIou, val_hist = validate(model, val_loader, criterion)\n",
    "\n",
    "        print(f'[Validation Loss] : {val_loss:.6f} [mIoU]: {100.*val_mIou:.2f}%')\n",
    "\n",
    "        # log_confusion_matrix(\"Confusion Matrix - Validate\", val_hist, \"epoch/validate_confusion_matrix\", \"epoch/step\", epoch)\n",
    "        run.log({\n",
    "                \"epoch/step\": epoch,\n",
    "                \"epoch/val_loss\": val_loss,\n",
    "                \"epoch/val_mIou\": 100*val_mIou\n",
    "            },\n",
    "            commit=True\n",
    "        )\n",
    "\n",
    "\n",
    "        if epoch % 2 == 0 or epoch == end_epoch:\n",
    "            checkpoint = {\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"train_step\": train_step,\n",
    "                \"validate_step\": val_step,\n",
    "            }\n",
    "\n",
    "            file_name = f\"{run_id}_epoch_{epoch}.pth\"\n",
    "\n",
    "            # TODO: add some tables to artifact to enable comparisons\n",
    "\n",
    "            # Saving the progress\n",
    "            file_path = os.path.join(models_root_dir, file_name)\n",
    "            torch.save(checkpoint, file_path)\n",
    "\n",
    "            print(f\"Model saved to {file_path}\")\n",
    "\n",
    "            artifact = wandb.Artifact(name=run_id, type=\"model\")\n",
    "            artifact.add_file(file_path)\n",
    "\n",
    "            run.log_artifact(artifact, aliases=[\"latest\", f\"epoch_{epoch}\"])\n",
    "\n",
    "        if (epoch % 10) == 0:\n",
    "            log_confusion_matrix(\"Confusion Matrix - Train\", train_hist, \"epoch/train_confusion_matrix\", \"epoch/step\", epoch)\n",
    "            log_confusion_matrix(\"Confusion Matrix - Validate\", val_hist, \"epoch/validate_confusion_matrix\", \"epoch/step\", epoch)\n",
    "\n",
    "    # TODO: need to check if works\n",
    "    run.config[\"end_epoch\"] = min(end_epoch, run.config[\"end_epoch\"])\n",
    "\n",
    "    run.log({\n",
    "        \"model/flops\": num_flops(model, 512, 1024),\n",
    "        \"model/latency\": latency(model, 512, 1024)\n",
    "    })\n",
    "\n",
    "    run.finish()\n",
    "\n",
    "# wandb.finish()\n",
    "pipeline()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
