{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWIvDpquM-Tl"
      },
      "source": [
        "# Preparations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision.transforms import v2\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models\n",
        "import torch.optim as optim\n",
        "\n",
        "import wandb\n",
        "\n",
        "from PIL import Image\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "from tqdm.notebook import tqdm\n",
        "import time\n",
        "import cv2\n",
        "import shutil\n",
        "\n",
        "# !pip -q install -U fvcore\n",
        "from fvcore.nn import FlopCountAnalysis #, flop_count_table\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Tuple\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')\n",
        "\n",
        "from utils import *\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ia5-WeZ0op3"
      },
      "source": [
        "### Variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Dataset settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "randomcrop = RandomCrop((512, 1024))\n",
        "\n",
        "s = 0.4\n",
        "source_augmentation = v2.Compose([\n",
        "    v2.RandomApply([v2.GaussianBlur(7)],p=0.5),\n",
        "    v2.RandomApply([v2.ColorJitter(brightness=s, contrast=s, saturation=s, hue=s)],p=0.5),\n",
        "    # v2.RandomApply([v2.GaussianNoise(mean=0.0, sigma=0.4)],p=0.5)\n",
        "])\n",
        "\n",
        "s = 0.1\n",
        "target_augmentation = v2.Compose([\n",
        "    v2.RandomApply([v2.GaussianBlur(3)],p=0.5),\n",
        "    v2.RandomApply([v2.ColorJitter(brightness=s, contrast=s, saturation=s, hue=s)],p=0.5),\n",
        "    # v2.RandomApply([v2.GaussianNoise(mean=0.0, sigma=0.03)],p=0.5)\n",
        "])\n",
        "\n",
        "s = 0.25\n",
        "mixed_augmentation = target_augmentation = v2.Compose([\n",
        "    v2.RandomApply([v2.GaussianBlur(7)],p=0.5),\n",
        "    v2.RandomApply([v2.ColorJitter(brightness=s, contrast=s, saturation=s, hue=s)],p=0.5),\n",
        "    # v2.RandomApply([v2.GaussianNoise(mean=0.0, sigma=0.03)],p=0.5)\n",
        "])\n",
        "\n",
        "source_augmentation = nn.Identity()\n",
        "target_augmentation = nn.Identity()\n",
        "mixed_augmentation = nn.Identity()\n",
        "\n",
        "resolution_source_dataset = (720, 1280)\n",
        "resolution_target_dataset = (512, 1024)\n",
        "\n",
        "toTensor = v2.ToTensor()\n",
        "\n",
        "mean=[0.485, 0.456, 0.406]\n",
        "std=[0.229, 0.224, 0.225]\n",
        "\n",
        "normalize = v2.Normalize(mean=mean, std=std)\n",
        "\n",
        "img_transform_target = v2.Compose([\n",
        "    toTensor,\n",
        "    v2.Resize(resolution_target_dataset),\n",
        "    # normalize\n",
        "])\n",
        "label_transform_target = v2.Compose([\n",
        "    toTensor,\n",
        "    v2.Resize(resolution_target_dataset, interpolation=v2.InterpolationMode.NEAREST),\n",
        "])\n",
        "\n",
        "img_transform_source = v2.Compose([\n",
        "    toTensor,\n",
        "    v2.Resize(resolution_source_dataset),\n",
        "    # normalize\n",
        "])\n",
        "\n",
        "label_transform_source = v2.Compose([\n",
        "    toTensor,\n",
        "    v2.Resize(resolution_source_dataset, interpolation=v2.InterpolationMode.NEAREST),\n",
        "])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5n1Dm1A0gjl"
      },
      "source": [
        "#### Logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_HX9maPV0iVa"
      },
      "outputs": [],
      "source": [
        "ENABLE_PRINT = False\n",
        "ENABLE_WANDB_LOG = True\n",
        "log_per_epoch = 20\n",
        "n_classes = 19\n",
        "\n",
        "train_step = 0\n",
        "val_step = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jntcuf0zNwaq"
      },
      "source": [
        "#### Device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ie8Own_ANu_t",
        "outputId": "5bd867f3-0d35-432f-8b08-03bcaa2cd14e"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "# device = torch.device('cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Downloads"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLfNdNzyMyxn"
      },
      "source": [
        "## CityScapes download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136,
          "referenced_widgets": [
            "93c53ea0fe8c4fdd8a97ba1a5ee71af3",
            "d0b6dc54747d46b5bf96f4b795b5627d",
            "940ea8bba0c64f78961399d4855eeace",
            "3d50ec68a6e6484dbef4b94d9b9bfd2a",
            "26559a2ed8ae4244b098950164fa0eb5",
            "4899af448b984d12bcc663899963447d",
            "166da3ac145f40d894f626ab60f82774",
            "e1d31ad13ffb4ad0a0a658c3f363eee6",
            "43d0c4a9221d45f9ba3a9c67eed8123d",
            "c62c4f0186b74b999cc2c7c8610f76de",
            "cbc43d6d534e4d439f95a1d694f7ac00"
          ]
        },
        "id": "B5p-mIhc2uIx",
        "outputId": "d689ad53-0a48-4ba5-d023-c1451dd7dc1e"
      },
      "outputs": [],
      "source": [
        "# !pip install -q gdown\n",
        "\n",
        "# file_id = \"1MI8QsvjW0R6WDJiL49L7sDGpPWYAQB6O\"\n",
        "# !gdown https://drive.google.com/uc?id={file_id}\n",
        "\n",
        "# pretty_extract(\"Cityscapes.zip\", \".\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAxaUMhvM4E4"
      },
      "source": [
        "## GTA5 download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136,
          "referenced_widgets": [
            "f5f4c5d20eac43a7bee11061e5d36933",
            "d39adf153c8445da9829b09973db1553",
            "ca53b671cecf4f09b535f69950e76158",
            "b6438ae4a5e4474bbbfa7dbb7c806a16",
            "219f47ce4b8d4f3dac47e21d4d95c63f",
            "d143fab025d84a85abfe83c0767ebc28",
            "64cd3c2cc1d34100b8d861d14bcd6178",
            "581ce3cc835c4576a39255978c8526cb",
            "e59f8ab1c510475fb8c4d50a00a1df87",
            "69c6f32570574572a57963c78fb9d864",
            "6684ff67a8f14b94819fb4b3718be137"
          ]
        },
        "id": "O44WrKuLMxd4",
        "outputId": "e596a25a-e71c-492b-cf76-6020d898c5c4"
      },
      "outputs": [],
      "source": [
        "# !pip install -q gdown\n",
        "\n",
        "# file_id = \"1PWavqXDxuifsyYvs2PFua9sdMl0JG8AE\"\n",
        "# !gdown https://drive.google.com/uc?id={file_id}\n",
        "\n",
        "# pretty_extract(\"Gta5_extended.zip\", \"./Gta5_extended\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEx91uPqlFv3"
      },
      "source": [
        "## DeepLabV2 model weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52R4dm2elKPM"
      },
      "outputs": [],
      "source": [
        "# !pip install -q gdown\n",
        "\n",
        "# file_id = \"1KgYgBTmvq7UcBwKui2b4TomnbTmzJMBf\"\n",
        "# !gdown https://drive.google.com/uc?id={file_id}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets.cityscapes import CityScapes\n",
        "from datasets.gta5 import GTA5_dataset_splitter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from models.bisenet.build_bisenet import *\n",
        "from models.deeplabv2.deeplabv2 import *\n",
        "from models.hrda.build_hrda import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bisenet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class BiSeNetloss(nn.Module):\n",
        "    def __init__(self, ignore_index=255):\n",
        "        super().__init__()\n",
        "        self.ignore_index = ignore_index\n",
        "        self.ce_loss = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, outputs, cx1_sup, cx2_sup, label):\n",
        "        # NOTE: the auxilliary losses may not be used\n",
        "\n",
        "        loss = self.ce_loss(outputs, label) + self.ce_loss(cx1_sup, label) + self.ce_loss(cx2_sup, label)\n",
        "\n",
        "        return loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## DACS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# NOTE: threshold = 0.968, from https://github.com/vikolss/DACS/blob/cc6a87f23b1c81ae32edad767b9772258774a974/trainUDA.py#L467\n",
        "class DACSloss(nn.Module):\n",
        "    def __init__(self, threshold = 0.968, ignore_index=255):\n",
        "        super().__init__()\n",
        "        self.ignore_index = ignore_index\n",
        "        self.threshold = threshold\n",
        "        self.ce_loss = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, outputs_source, cx1_sup_src, cx2_sup_src, label_source, \n",
        "                outputs_mixed, cx1_sup_mixed, cx2_sup_mixed, label_mixed):\n",
        "        \"\"\"\n",
        "        outputs_src: [B, C, H, W]\n",
        "        targets: [B, H, W]\n",
        "        outputs_tgt: [B, C, H, W]\n",
        "        \"\"\"\n",
        "        # Cross entropy on source\n",
        "        # NOTE: the auxilliary losses may not be used\n",
        "\n",
        "        bisenet_loss = BiSeNetloss()\n",
        "\n",
        "        if cx1_sup_src is not None:\n",
        "            l1 = bisenet_loss(outputs_source, cx1_sup_src, cx2_sup_src, label_source)\n",
        "        else:\n",
        "            l1 = self.ce_loss(torch.argmax(outputs_source, 1), label_source)\n",
        "\n",
        "        if cx1_sup_mixed is not None:\n",
        "            l2 = bisenet_loss(outputs_mixed, cx1_sup_mixed, cx2_sup_mixed, label_mixed)\n",
        "        else:\n",
        "            l2 = self.ce_loss(torch.argmax(outputs_mixed, 1), label_mixed)\n",
        "            \n",
        "        max_probs, _ = torch.softmax(outputs_mixed, 1).max(1)\n",
        "\n",
        "        f = (max_probs >= self.threshold).float()\n",
        "        if f.size(-1) > 0:\n",
        "            lambda_ = f.mean().item()\n",
        "        else:\n",
        "            lambda_ = 0\n",
        "\n",
        "        loss = l1 + lambda_*l2\n",
        "\n",
        "        return loss, lambda_\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## FDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def charbonnier_penalty(x, eta = 0.5):\n",
        "    return (x**2 + 0.001**2)**eta\n",
        "\n",
        "class FDAloss(nn.Module):\n",
        "    def __init__(self, lambda_entropy=0.01, ignore_index=255):\n",
        "        super().__init__()\n",
        "        self.lambda_entropy = lambda_entropy\n",
        "        self.ignore_index = ignore_index\n",
        "        self.ce_loss = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, outputs_src, targets, outputs_tgt):\n",
        "        \"\"\"\n",
        "        outputs_src: [B, C, H, W]\n",
        "        targets: [B, H, W]\n",
        "        outputs_tgt: [B, C, H, W]\n",
        "        \"\"\"\n",
        "        # Cross entropy on source\n",
        "        loss_ce = self.ce_loss(outputs_src, targets)\n",
        "\n",
        "        # entropy on target\n",
        "        probs = F.softmax(outputs_tgt, dim=1)                    # [B, C, H, W]\n",
        "        log_probs = F.log_softmax(outputs_tgt, dim=1)            # [B, C, H, W]\n",
        "        entropy = -torch.sum(probs * log_probs, dim=1)           # [B, H, W]\n",
        "\n",
        "        if targets.shape == entropy.shape:\n",
        "            valid_mask = (targets != self.ignore_index).float()  # [B, H, W]\n",
        "            entropy = entropy * valid_mask\n",
        "\n",
        "        entropy_per_image = charbonnier_penalty(entropy.view(entropy.size(0), -1).sum(dim=1))  # [B]\n",
        "\n",
        "        # Totale\n",
        "        loss_ent = entropy_per_image.sum()  # somma sui batch\n",
        "        total_loss = loss_ce + self.lambda_entropy * loss_ent\n",
        "\n",
        "        return total_loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9ffq97eM8Rg"
      },
      "source": [
        "# Train/Val loops"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0KuuGIWNAbF"
      },
      "source": [
        "## Train Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train step 2/3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-slUs8vsNAEe"
      },
      "outputs": [],
      "source": [
        "def train3(model:nn.Module, train_loader:DataLoader, criterion:nn.Module, optimizer:optim.Optimizer) -> tuple[float, float, torch.Tensor, torch.Tensor]:\n",
        "    global device\n",
        "    global n_classes\n",
        "    global ENABLE_PRINT\n",
        "    global ENABLE_WANDB_LOG\n",
        "    global train_step\n",
        "    global log_per_epoch\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    num_batch = len(train_loader)\n",
        "    chunk_batch = num_batch//log_per_epoch+1\n",
        "\n",
        "    num_sample = len(train_loader.dataset)\n",
        "    seen_sample = 0\n",
        "\n",
        "    train_loss = 0.0\n",
        "    train_hist = torch.zeros((n_classes,n_classes), device=device)\n",
        "\n",
        "    for batch_idx, (inputs, _, targets) in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
        "        batch_size = inputs.size(0)\n",
        "        seen_sample += batch_size\n",
        "\n",
        "        inputs, targets = inputs.to(device), targets.squeeze().to(device)\n",
        "\n",
        "        if isinstance(model, BiSeNetWithHRDA):\n",
        "            lr_img, lr_label, hr_img, hr_label, coords = model.hrda_crop(normalize(inputs), targets)\n",
        "            outputs, (lr_out, hr_out) = model.hrda_forward(lr_img, hr_img, coords)\n",
        "            loss = model.hrda_loss(criterion, outputs, targets, lr_out, lr_label, hr_out, hr_label)\n",
        "        else:\n",
        "            outputs, cx1_sup, cx2_sup = model(normalize(inputs))\n",
        "            if cx1_sup is not None and cx2_sup is not None:\n",
        "                loss = criterion(outputs, cx1_sup, cx2_sup, targets)\n",
        "            else:\n",
        "                loss = criterion(outputs, targets)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        predicted = outputs.argmax(1)\n",
        "\n",
        "        hist_batch = fast_hist_cuda(targets.view(-1).detach(), predicted.view(-1).detach(), n_classes)\n",
        "\n",
        "        train_loss += loss.item() * batch_size\n",
        "        train_hist += hist_batch\n",
        "\n",
        "        if ((batch_idx+1) % chunk_batch) == 0:\n",
        "            iou_batch = per_class_iou_cuda(hist_batch)\n",
        "            if ENABLE_PRINT:\n",
        "                    print(f'Training [{seen_sample}/{num_sample} ({100. * seen_sample / num_sample:.0f}%)]')\n",
        "                    print(f'\\tLoss: {loss.item():.6f}')\n",
        "                    print(f\"\\tmIoU: {100.*iou_batch[iou_batch > 0].mean():.4f}\")\n",
        "\n",
        "            if ENABLE_WANDB_LOG:\n",
        "                wandb.log({\n",
        "                        \"train/step\": train_step,\n",
        "                        \"train/batch_loss\": loss.item(),\n",
        "                        \"train/batch_mIou\": 100.*iou_batch[iou_batch > 0].mean()\n",
        "                    },\n",
        "                    commit=True,\n",
        "                )\n",
        "                train_step += 1\n",
        "\n",
        "    train_loss = train_loss / seen_sample\n",
        "\n",
        "    train_iou_class = per_class_iou_cuda(train_hist)\n",
        "    train_mIou = train_iou_class[train_iou_class > 0].mean().item()\n",
        "\n",
        "    return train_loss, train_mIou, train_iou_class, train_hist \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train step 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_4A(model:nn.Module, source_loader:DataLoader, target_loader:DataLoader , criterion:FDAloss, optimizer:optim.Optimizer, beta: float = 0.01, n:int = 1, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) -> tuple[float, float, torch.Tensor, torch.Tensor]:\n",
        "    global device\n",
        "    global n_classes\n",
        "    global ENABLE_PRINT\n",
        "    global ENABLE_WANDB_LOG\n",
        "    global train_step\n",
        "    global log_per_epoch\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    num_batch = len(source_loader)\n",
        "    chunk_batch = num_batch//log_per_epoch+1\n",
        "\n",
        "    num_sample = len(source_loader.dataset)\n",
        "    seen_sample = 0\n",
        "\n",
        "    train_loss = 0.0\n",
        "    train_hist = torch.zeros((n_classes,n_classes)).to(device)\n",
        "\n",
        "    for batch_idx, (inputs_src, _, targets_src), (inputs_tgt, _, _) in tqdm(enumerate(zip(source_loader, target_loader)), total=min(len(source_loader), len(target_loader))):\n",
        "        inputs_src, inputs_tgt = FDA(inputs_src, inputs_tgt, beta, n, mean, std)\n",
        "\n",
        "        batch_size = inputs_src.size(0)\n",
        "        seen_sample += batch_size\n",
        "\n",
        "        inputs_src, targets_src = inputs_src.to(device), targets_src.squeeze(1).to(device)\n",
        "        inputs_tgt = inputs_tgt.to(device)\n",
        "\n",
        "        outputs_src, cx1_sup_src, cx2_sup_src = model(inputs_src)\n",
        "        outputs_tgt, cx1_sup_tgt, cx2_sup_tgt = model(inputs_tgt)\n",
        "\n",
        "        loss = criterion(outputs_src, targets_src, outputs_tgt)\n",
        "        loss += criterion(cx1_sup_src, targets_src, cx1_sup_tgt)\n",
        "        loss += criterion(cx2_sup_src, targets_src, cx2_sup_tgt)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        predicted_src = outputs_src.argmax(1)\n",
        "\n",
        "        hist_batch = torch.zeros((n_classes, n_classes)).to(device)\n",
        "\n",
        "        assert(predicted_src.shape == targets_src.shape)\n",
        "        hist_batch = fast_hist_cuda(targets_src.view(-1).detach(),predicted_src.view(-1).detach(),n_classes)\n",
        "\n",
        "        train_loss += loss.item() * batch_size\n",
        "        train_hist += hist_batch\n",
        "\n",
        "        if ((batch_idx+1) % chunk_batch) == 0:\n",
        "            iou_batch = per_class_iou_cuda(hist_batch)\n",
        "            if ENABLE_PRINT:\n",
        "                    print(f'Training [{seen_sample}/{num_sample} ({100. * seen_sample / num_sample:.0f}%)]')\n",
        "                    print(f'\\tLoss: {loss.item():.6f}')\n",
        "                    print(f\"\\tmIoU: {100.*iou_batch[iou_batch > 0].mean():.4f}\")\n",
        "\n",
        "            if ENABLE_WANDB_LOG:\n",
        "                wandb.log({\n",
        "                        \"train/step\": train_step,\n",
        "                        \"train/batch_loss\": loss.item(),\n",
        "                        \"train/batch_mIou\": 100.*iou_batch[iou_batch > 0].mean()\n",
        "                    },\n",
        "                    commit=True,\n",
        "                )\n",
        "                train_step += 1\n",
        "\n",
        "    train_loss = train_loss / seen_sample\n",
        "\n",
        "    train_iou_class = per_class_iou_cuda(train_hist)\n",
        "    train_mIou = train_iou_class[train_iou_class > 0].mean().item()\n",
        "\n",
        "    return train_loss, train_mIou, train_hist, train_iou_class\n",
        "\n",
        "def train_4B(model:nn.Module, source_loader:DataLoader, target_loader:DataLoader, criterion:DACSloss, optimizer:optim.Optimizer, classmixer:ClassMixer) -> tuple[float, float, torch.Tensor, torch.Tensor]:\n",
        "    global device\n",
        "    global n_classes\n",
        "    global ENABLE_PRINT\n",
        "    global ENABLE_WANDB_LOG\n",
        "    global train_step\n",
        "    global log_per_epoch\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    num_batch = len(source_loader)\n",
        "    chunk_batch = num_batch//log_per_epoch+1\n",
        "\n",
        "    num_sample = len(source_loader.dataset)\n",
        "    seen_sample = 0\n",
        "\n",
        "    train_loss = 0.0\n",
        "    train_hist = torch.zeros((n_classes,n_classes), device=device)\n",
        "    \n",
        "    for batch_idx, ((inputs_src, _, label_source), (inputs_target, _, _)) in tqdm(enumerate(zip(source_loader, target_loader)), total=min(len(source_loader), len(target_loader))):\n",
        "        inputs_src, label_source = inputs_src.to(device), label_source.squeeze().to(device)\n",
        "        inputs_target = inputs_target.to(device)        \n",
        "    \n",
        "        inputs_target = target_augmentation(inputs_target)\n",
        "\n",
        "        inputs_src, label_source = randomcrop(inputs_src, label_source)\n",
        "        inputs_target, _ = randomcrop(inputs_target, None)\n",
        "        \n",
        "        B = inputs_src.size(0) + inputs_target.size(0)\n",
        "        seen_sample += B\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs_target = model(normalize(inputs_target).detach())[0]\n",
        "        label_target = outputs_target.argmax(1).detach()\n",
        "\n",
        "        inputs_mixed, label_mixed = classmixer(inputs_src, inputs_target, label_source, label_target)\n",
        "\n",
        "        inputs_src = source_augmentation(inputs_src)\n",
        "        inputs_mixed = mixed_augmentation(inputs_mixed)\n",
        "\n",
        "        inputs_src = normalize(inputs_src)\n",
        "        inputs_mixed = normalize(inputs_mixed)\n",
        "\n",
        "        outputs_source, cx1_sup_src, cx2_sup_src = model(inputs_src.detach())\n",
        "        outputs_mixed, cx1_sup_mixed, cx2_sup_mixed = model(inputs_mixed.detach())\n",
        "        \n",
        "        # Loss calculation:\n",
        "        loss, lambda_ = criterion(outputs_source, cx1_sup_src, cx2_sup_src, label_source, outputs_mixed, cx1_sup_mixed, cx2_sup_mixed, label_mixed)\n",
        "        \n",
        "        predicted_source = outputs_source.argmax(1)\n",
        "        predicted_mixed = outputs_mixed.argmax(1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        y, y_pred = torch.reshape(torch.cat((label_source,      label_mixed)),     (-1,)),\\\n",
        "                    torch.reshape(torch.cat((predicted_source,  predicted_mixed)), (-1,)) \n",
        "\n",
        "        hist_batch = fast_hist_cuda(y, y_pred, n_classes)\n",
        "\n",
        "        train_loss += loss.item() * B\n",
        "        train_hist += hist_batch\n",
        "\n",
        "        if ((batch_idx+1) % chunk_batch) == 0:\n",
        "            iou_batch = per_class_iou_cuda(hist_batch)\n",
        "            if ENABLE_PRINT:\n",
        "                    print(f'Training [{seen_sample}/{num_sample} ({100. * seen_sample / num_sample:.0f}%)]')\n",
        "                    print(f'\\tLoss: {loss.item():.6f}')\n",
        "                    print(f\"\\tmIoU: {100.*iou_batch[iou_batch > 0].mean():.4f}\")\n",
        "                    print(f\"\\tunsupervised_confidence: {lambda_}\")\n",
        "\n",
        "            if ENABLE_WANDB_LOG:\n",
        "                wandb.log({\n",
        "                        \"train/step\": train_step,\n",
        "                        \"train/batch_loss\": loss.item(),\n",
        "                        \"train/batch_mIou\": 100.*iou_batch[iou_batch > 0].mean(),\n",
        "                        \"train/unsupervised_confidence\": lambda_\n",
        "                    }\n",
        "                )\n",
        "\n",
        "                train_step += 1\n",
        "\n",
        "    train_loss = train_loss / seen_sample\n",
        "\n",
        "    train_iou_class = per_class_iou_cuda(train_hist)\n",
        "    train_mIou = train_iou_class[train_iou_class > 0].mean().item()\n",
        "\n",
        "    return train_loss, train_mIou, train_iou_class, train_hist\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train step 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train5_1(model:BiSeNetWithHRDA, source_loader:DataLoader, criterion:nn.CrossEntropyLoss, optimizer:optim.Optimizer) -> tuple[float, float, torch.Tensor, torch.Tensor]:\n",
        "    global device\n",
        "    global n_classes\n",
        "    global ENABLE_PRINT\n",
        "    global ENABLE_WANDB_LOG\n",
        "    global train_step\n",
        "    global log_per_epoch\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    num_batch = len(source_loader)\n",
        "    chunk_batch = num_batch//log_per_epoch+1\n",
        "\n",
        "    num_sample = len(source_loader.dataset)\n",
        "    seen_sample = 0\n",
        "\n",
        "    train_loss = 0.0\n",
        "    train_hist = torch.zeros((n_classes,n_classes), device=device)\n",
        "    \n",
        "    for batch_idx, (inputs_src, _, label_source) in tqdm(enumerate(source_loader), total=len(source_loader)):\n",
        "        inputs_src, label_source = randomcrop(inputs_src, label_source)\n",
        "        \n",
        "        B = inputs_src.size(0)\n",
        "        seen_sample += B\n",
        "\n",
        "        inputs_src, label_source = inputs_src.to(device), label_source.squeeze().to(device)\n",
        "\n",
        "        inputs_src = source_augmentation(inputs_src)\n",
        "\n",
        "        inputs_src = normalize(inputs_src)\n",
        "        \n",
        "        lr_img_src, lr_label_src, hr_img_src, hr_label_src, coords_src = model.hrda_crop(inputs_src, label_source)\n",
        "        outputs_source, (lr_out_src, hr_out_src) = model.hrda_forward(lr_img_src, hr_img_src, coords_src)\n",
        "\n",
        "        loss = model.hrda_loss(criterion, outputs_source, label_source, lr_out_src, lr_label_src, hr_out_src, hr_label_src)\n",
        "  \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        predicted_source = outputs_source.argmax(1)\n",
        "\n",
        "        y, y_pred = torch.reshape(label_source.detach(), (-1,)),\\\n",
        "                    torch.reshape(predicted_source.detach(), (-1,)) \n",
        "\n",
        "        hist_batch = fast_hist_cuda(y, y_pred, n_classes)\n",
        "\n",
        "        train_loss += loss.item() * B\n",
        "        train_hist += hist_batch\n",
        "\n",
        "        if ((batch_idx+1) % chunk_batch) == 0:\n",
        "            iou_batch = per_class_iou_cuda(hist_batch)\n",
        "            if ENABLE_PRINT:\n",
        "                    print(f'Training [{seen_sample}/{num_sample} ({100. * seen_sample / num_sample:.0f}%)]')\n",
        "                    print(f'\\tLoss: {loss.item():.6f}')\n",
        "                    print(f\"\\tmIoU: {100.*iou_batch[iou_batch > 0].mean():.4f}\")\n",
        "                    # print(f\"\\tunsupervised_confidence: {lambda_}\")\n",
        "\n",
        "            if ENABLE_WANDB_LOG:\n",
        "                wandb.log({\n",
        "                        \"train/step\": train_step,\n",
        "                        \"train/batch_loss\": loss.item(),\n",
        "                        \"train/batch_mIou\": 100.*iou_batch[iou_batch > 0].mean(),\n",
        "                        # \"train/unsupervised_confidence\": lambda_\n",
        "                    }\n",
        "                )\n",
        "\n",
        "                train_step += 1\n",
        "\n",
        "    train_loss = train_loss / seen_sample\n",
        "\n",
        "    train_iou_class = per_class_iou_cuda(train_hist)\n",
        "    train_mIou = train_iou_class[train_iou_class > 0].mean().item()\n",
        "\n",
        "    return train_loss, train_mIou, train_iou_class, train_hist\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train5_2(model:BiSeNetWithHRDA, source_loader:DataLoader, target_loader:DataLoader, criterion:nn.CrossEntropyLoss, optimizer:optim.Optimizer, classmixer:ClassMixer) -> tuple[float, float, torch.Tensor, torch.Tensor]:\n",
        "    global device\n",
        "    global n_classes\n",
        "    global ENABLE_PRINT\n",
        "    global ENABLE_WANDB_LOG\n",
        "    global train_step\n",
        "    global log_per_epoch\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    num_batch = len(source_loader)\n",
        "    chunk_batch = num_batch//log_per_epoch+1\n",
        "\n",
        "    num_sample = len(source_loader.dataset)\n",
        "    seen_sample = 0\n",
        "\n",
        "    train_loss = 0.0\n",
        "    train_hist = torch.zeros((n_classes,n_classes), device=device)\n",
        "\n",
        "    threshold = 0.968\n",
        "    \n",
        "    # NOTE: threshold = 0.968, from https://github.com/vikolss/DACS/blob/cc6a87f23b1c81ae32edad767b9772258774a974/trainUDA.py#L467\n",
        "    for batch_idx, ((inputs_src, _, label_source), (inputs_target, _, _)) in tqdm(enumerate(zip(source_loader, target_loader)), total=min(len(source_loader), len(target_loader))):\n",
        "        inputs_src, label_source = inputs_src.to(device), label_source.squeeze().to(device)\n",
        "        inputs_target = inputs_target.to(device)        \n",
        "        \n",
        "        inputs_target = target_augmentation(inputs_target)\n",
        "\n",
        "        inputs_src, label_source = randomcrop(inputs_src, label_source)\n",
        "        inputs_target, _ = randomcrop(inputs_target, None)\n",
        "        \n",
        "        B = inputs_src.size(0) + inputs_target.size(0)\n",
        "        seen_sample += B\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs_target = model(normalize(inputs_target).detach())[0]\n",
        "        label_target = outputs_target.argmax(1)\n",
        "\n",
        "        inputs_mixed, label_mixed = classmixer(inputs_src, inputs_target, label_source, label_target)\n",
        "\n",
        "        inputs_src = source_augmentation(inputs_src)\n",
        "        inputs_mixed = mixed_augmentation(inputs_mixed)\n",
        "\n",
        "        inputs_src = normalize(inputs_src)\n",
        "        inputs_mixed = normalize(inputs_mixed)\n",
        " \n",
        "        lr_img_src, lr_label_src, hr_img_src, hr_label_src, coords_src = model.hrda_crop(inputs_src, label_source)\n",
        "        outputs_source, (lr_out_src, hr_out_src) = model.hrda_forward(lr_img_src, hr_img_src, coords_src)\n",
        "\n",
        "        lr_img_m, lr_label_m, hr_img_m, hr_label_m, coords_m = model.hrda_crop(inputs_mixed, label_mixed)\n",
        "        outputs_m, (lr_out_m, hr_out_m) = model.hrda_forward(lr_img_m, hr_img_m, coords_m)\n",
        "        \n",
        "        # Loss calculation:\n",
        "        l1 = model.hrda_loss(criterion, outputs_source, label_source, lr_out_src, lr_label_src, hr_out_src, hr_label_src)\n",
        "        l2 = model.hrda_loss(criterion, outputs_m, label_mixed, lr_out_m, lr_label_m, hr_out_m, hr_label_m)\n",
        "\n",
        "        max_probs, predicted_m = torch.softmax(outputs_m, 1).max(1)\n",
        "        predicted_src = torch.argmax(outputs_source, 1)\n",
        "\n",
        "        f = (max_probs >= threshold).float()\n",
        "        if f.size(-1) > 0:\n",
        "            lambda_ = f.mean().item()\n",
        "        else:\n",
        "            lambda_ = 0\n",
        "\n",
        "        loss = l1 + lambda_*l2\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        y, y_pred = torch.reshape(torch.cat((label_source,   label_mixed)),     (-1,)),\\\n",
        "                    torch.reshape(torch.cat((predicted_src,  predicted_m)),     (-1,)) \n",
        "\n",
        "        hist_batch = fast_hist_cuda(y, y_pred, n_classes)\n",
        "\n",
        "        train_loss += loss.item() * B\n",
        "        train_hist += hist_batch\n",
        "\n",
        "        if ((batch_idx+1) % chunk_batch) == 0:\n",
        "            iou_batch = per_class_iou_cuda(hist_batch)\n",
        "            if ENABLE_PRINT:\n",
        "                    print(f'Training [{seen_sample}/{num_sample} ({100. * seen_sample / num_sample:.0f}%)]')\n",
        "                    print(f'\\tLoss: {loss.item():.6f}')\n",
        "                    print(f\"\\tmIoU: {100.*iou_batch[iou_batch > 0].mean():.4f}\")\n",
        "                    print(f\"\\tunsupervised_confidence: {lambda_}\")\n",
        "\n",
        "            if ENABLE_WANDB_LOG:\n",
        "                wandb.log({\n",
        "                        \"train/step\": train_step,\n",
        "                        \"train/batch_loss\": loss.item(),\n",
        "                        \"train/batch_mIou\": 100.*iou_batch[iou_batch > 0].mean(),\n",
        "                        \"train/unsupervised_confidence\": lambda_\n",
        "                    }\n",
        "                )\n",
        "\n",
        "                train_step += 1\n",
        "\n",
        "    train_loss = train_loss / seen_sample\n",
        "\n",
        "    train_iou_class = per_class_iou_cuda(train_hist)\n",
        "    train_mIou = train_iou_class[train_iou_class > 0].mean().item()\n",
        "\n",
        "    return train_loss, train_mIou, train_iou_class, train_hist\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ON-XdXCkNCrn"
      },
      "source": [
        "## Validation loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13a6MV00QIot"
      },
      "outputs": [],
      "source": [
        "def validate(model:nn.Module, val_loader:DataLoader, criterion:nn.Module) -> tuple[float, float, torch.Tensor, torch.Tensor]:\n",
        "    global device\n",
        "    global n_classes\n",
        "    global ENABLE_PRINT\n",
        "    global ENABLE_WANDB_LOG\n",
        "    global val_step\n",
        "    global log_per_epoch\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    num_batch = len(val_loader)\n",
        "    chunk_batch = num_batch//log_per_epoch+1\n",
        "\n",
        "    num_sample = len(val_loader.dataset)\n",
        "    seen_sample = 0\n",
        "    chunk_sample = 0\n",
        "\n",
        "    val_loss = 0.0\n",
        "    chunk_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        val_hist = torch.zeros((n_classes,n_classes), device=device)\n",
        "        chunk_hist = torch.zeros((n_classes,n_classes), device=device)\n",
        "    \n",
        "        for batch_idx, (inputs, _, targets) in tqdm(enumerate(val_loader), total=num_batch):\n",
        "            inputs, targets = randomcrop(inputs, targets)\n",
        "\n",
        "            B = inputs.size(0)\n",
        "        \n",
        "            inputs, targets = inputs.to(device), targets.squeeze().to(device)\n",
        "\n",
        "            if isinstance(model, BiSeNetWithHRDA):\n",
        "                lr_img, lr_label, hr_img, hr_label, coords = model.hrda_crop(normalize(inputs), targets)\n",
        "                outputs, (lr_out, hr_out) = model.hrda_forward(lr_img, hr_img, coords)\n",
        "                \n",
        "                # loss = model.hrda_loss(criterion, outputs, targets, lr_out, lr_label, hr_out, hr_label)\n",
        "            else:\n",
        "                outputs = model(normalize(inputs))\n",
        "                \n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            predicted = outputs.argmax(1)\n",
        "\n",
        "            hist_batch = fast_hist_cuda(torch.reshape(targets.detach(), (-1,)), torch.reshape(predicted.detach(), (-1,)), n_classes)\n",
        "\n",
        "            chunk_sample += B\n",
        "            chunk_loss += loss.item() * B\n",
        "            chunk_hist += hist_batch\n",
        "\n",
        "            if ((batch_idx+1) % chunk_batch) == 0:\n",
        "                seen_sample += chunk_sample\n",
        "                val_loss += chunk_loss\n",
        "                val_hist += chunk_hist\n",
        "\n",
        "                if ENABLE_PRINT:\n",
        "                    iou_batch = per_class_iou_cuda(hist_batch)\n",
        "                    print(f'Validation [{seen_sample}/{num_sample} ({100. * seen_sample / num_sample:.0f}%)]')\n",
        "                    print(f'\\tLoss: {loss.item():.6f}')\n",
        "                    print(f\"\\tmIoU: {100.*iou_batch[iou_batch > 0].mean():.4f}\")\n",
        "\n",
        "                if ENABLE_WANDB_LOG:\n",
        "                    iou_batch = per_class_iou_cuda(chunk_hist)\n",
        "                    wandb.log({\n",
        "                            \"validate/step\": val_step,\n",
        "                            \"validate/batch_loss\": chunk_loss/chunk_sample,\n",
        "                            \"validate/batch_mIou\": 100.*iou_batch[iou_batch > 0].mean()\n",
        "                        }\n",
        "                    )\n",
        "\n",
        "                    val_step += 1\n",
        "\n",
        "                chunk_sample = 0\n",
        "                chunk_loss = 0.0\n",
        "                chunk_hist *= 0.0\n",
        "\n",
        "        if chunk_sample > 0:\n",
        "            seen_sample += chunk_sample\n",
        "            val_loss += chunk_loss\n",
        "            val_hist += chunk_hist\n",
        "\n",
        "            if ENABLE_PRINT:\n",
        "                iou_batch = per_class_iou_cuda(hist_batch)\n",
        "                print(f'Validation [{seen_sample}/{num_sample} ({100. * seen_sample / num_sample:.0f}%)]')\n",
        "                print(f'\\tLoss: {loss.item():.6f}')\n",
        "                print(f\"\\tmIoU: {100.*iou_batch[iou_batch > 0].mean():.4f}\")\n",
        "\n",
        "            if ENABLE_WANDB_LOG:\n",
        "                iou_batch = per_class_iou_cuda(chunk_hist)\n",
        "                wandb.log({\n",
        "                        \"validate/step\": val_step,\n",
        "                        \"validate/batch_loss\": chunk_loss/chunk_sample,\n",
        "                        \"validate/batch_mIou\": 100.*iou_batch[iou_batch > 0].mean()\n",
        "                    }\n",
        "                )\n",
        "\n",
        "                val_step += 1\n",
        "\n",
        "    val_loss = val_loss / seen_sample\n",
        "\n",
        "    val_iou_class = per_class_iou_cuda(val_hist)\n",
        "    val_mIou = val_iou_class[val_iou_class > 0].mean().item()\n",
        "\n",
        "    return val_loss, val_mIou, val_iou_class, val_hist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgPi7OfDNLJX"
      },
      "source": [
        "# Machine learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 825
        },
        "id": "AgetZKrsHlEk",
        "outputId": "8c53d02d-3494-46e6-b975-a63d9dce17ac"
      },
      "outputs": [],
      "source": [
        "def pipeline():\n",
        "    global device\n",
        "    global n_classes\n",
        "    global ENABLE_PRINT\n",
        "    global ENABLE_WANDB_LOG\n",
        "    global train_step\n",
        "    global val_step\n",
        "    global log_per_epoch\n",
        "\n",
        "    ENABLE_PRINT = False\n",
        "    ENABLE_WANDB_LOG = True\n",
        "    train_step = 0\n",
        "    val_step = 0\n",
        "    log_per_epoch = 20\n",
        "\n",
        "    models_root_dir = \"./models\"\n",
        "    # !rm -r {models_root_dir}\n",
        "    # !rd /s /q {models_root_dir}\n",
        "    !mkdir {models_root_dir}\n",
        "\n",
        "    B = 4\n",
        "    n_classes = 19\n",
        "\n",
        "    backbone = \"...\"\n",
        "    context_path = \"...\"\n",
        "    dataset = \"...\"\n",
        "\n",
        "    start_epoch = 0 # <--------- Last epoch that has been completed (in this script i will perform from the start+1 to end)\n",
        "    end_epoch = 50\n",
        "    max_epoch = 50\n",
        "\n",
        "    assert start_epoch < end_epoch <= max_epoch, \"Check your start/end/max epoch settings.\"\n",
        "\n",
        "    init_lr=2.5e-2\n",
        "    lr_decay_iter = 1\n",
        "    momentum=0.9\n",
        "    weight_decay=5e-4\n",
        "    n=2\n",
        "    beta = 0.01\n",
        "    lambda_entropy = 0.01\n",
        "    eta = 2\n",
        "    s = 0.5\n",
        "    rcs = False\n",
        "\n",
        "    # Dataset objects\n",
        "    if dataset == \"Cityscapes\":\n",
        "        data_train = CityScapes(\"./Cityscapes/Cityspaces\", split=\"train\", transform=img_transform_target, target_transform=label_transform_target)\n",
        "        data_val = CityScapes(\"./Cityscapes/Cityspaces\", split=\"val\", transform=img_transform_target, target_transform=label_transform_target)\n",
        "\n",
        "        train_loader = DataLoader(data_train, batch_size=B, shuffle=True)\n",
        "        val_loader = DataLoader(data_val, batch_size=B, shuffle=True)\n",
        "    elif dataset == \"GTA5\":\n",
        "        data_train, data_val = GTA5_dataset_splitter(\"./Gta5_extended\", train_split_percent=0.8, split_seed=42, transform=img_transform_source, target_transform=label_transform_source)\n",
        "        \n",
        "        train_loader = DataLoader(data_train, batch_size=B, shuffle=True)\n",
        "        val_loader = DataLoader(data_val, batch_size=B, shuffle=True)\n",
        "    elif dataset == \"Augmentation\":\n",
        "        data_train, _ = GTA5_dataset_splitter(\"./Gta5_extended\", train_split_percent=1, split_seed=42, augment=True, rcs=rcs, transform=img_transform_source, target_transform=label_transform_source)\n",
        "        data_val = CityScapes(\"./Cityscapes/Cityspaces\", split=\"val\", transform=img_transform_target, target_transform=label_transform_target)\n",
        "\n",
        "        train_loader = DataLoader(data_train, batch_size=B, shuffle=True)\n",
        "        val_loader = DataLoader(data_val, batch_size=B, shuffle=True)\n",
        "    elif dataset == \"Mixed\":\n",
        "        city_val = CityScapes(\"./Cityscapes/Cityspaces\", split=\"val\", transform=img_transform_target, target_transform=label_transform_target)\n",
        "        city_train = CityScapes(\"./Cityscapes/Cityspaces\", split=\"train\", transform=img_transform_target, target_transform=label_transform_target)\n",
        "        gta_train, _ = GTA5_dataset_splitter(\"./Gta5_extended\", 1.0, split_seed=42, augment=False, rcs=rcs, transform=img_transform_source, target_transform=label_transform_source)\n",
        "\n",
        "        source_loader = DataLoader(gta_train, batch_size=B, shuffle=True)\n",
        "        target_loader = DataLoader(city_train, batch_size=B, shuffle=True)\n",
        "        val_loader = DataLoader(city_val, batch_size=B, shuffle=True)\n",
        "    else:\n",
        "        raise Exception(\"Wrong dataset name\")\n",
        "\n",
        "    # Architecture\n",
        "    if backbone == \"BiSeNet\":\n",
        "        model = BiSeNet(n_classes, context_path).to(device)\n",
        "        architecture = backbone+\"-\"+context_path\n",
        "    elif backbone == \"DeepLab\":\n",
        "        model = get_deeplab_v2(num_classes=n_classes, pretrain=True).to(device)\n",
        "        architecture = backbone\n",
        "    elif backbone == \"BiSeNetHRDA\":\n",
        "        model = BiSeNetWithHRDA(n_classes, context_path, s).to(device)\n",
        "        architecture = backbone+\"-\"+context_path\n",
        "    else:\n",
        "        raise Exception(\"Wrong model name\")\n",
        "\n",
        "    # print(model)\n",
        "    \n",
        "    classmixer = ClassMixer(n_classes, 0.5, device=device)\n",
        "    \n",
        "    criterion_train = nn.CrossEntropyLoss(ignore_index=255)\n",
        "    # criterion_train = FDAloss(lambda_entropy=lambda_entropy, eta=eta, ignore_index=255)\n",
        "    # criterion_train = BiSeNetloss()\n",
        "    # criterion_train = DACSloss()\n",
        "\n",
        "    criterion_val = nn.CrossEntropyLoss(ignore_index=255)\n",
        "\n",
        "    optimizer = optim.SGD(model.parameters(), lr=init_lr, momentum=momentum, weight_decay=weight_decay, nesterov=True)\n",
        "\n",
        "    run_name = f\"...\"\n",
        "    run_detail = f\"{run_name}_{architecture}_{dataset}\"\n",
        "    run_id = None\n",
        "    \n",
        "    # Wandb setup and metrics\n",
        "    if ENABLE_WANDB_LOG:\n",
        "        run = wandb.init(\n",
        "            entity=\"Machine_learning_and_Deep_learning_labs\",\n",
        "            project=\"Semantic Segmentation\",\n",
        "            name=run_name,\n",
        "            id=run_id, # <--------------- If run already created (start_epoch > 0) decomment\n",
        "            resume=\"allow\", # <----------------  IMPORTANT CONFIG KEY\n",
        "            config={\n",
        "                \"initial_learning_rate\": init_lr,\n",
        "                \"lr_decay_iter\": lr_decay_iter,\n",
        "                \"momentum\": momentum,\n",
        "                \"weight_decay\": weight_decay,\n",
        "                \"architecture\": architecture,\n",
        "                \"dataset\": dataset,\n",
        "                \"start_epoch\": start_epoch,\n",
        "                \"end_epoch\": end_epoch,\n",
        "                \"max_epoch\": max_epoch,\n",
        "                \"batch\": B,\n",
        "                \"lr_scheduler\": \"poly\"\n",
        "            },\n",
        "        )\n",
        "        if run_id is None:\n",
        "            print(f\"\\nThe ID for this run is: {run.id} save this if first time creating the run, then use it in run creation: wandb.init(..., id = {run.id}, ...)\\n\")\n",
        "        \n",
        "        wandb.define_metric(\"epoch/step\")\n",
        "        wandb.define_metric(\"epoch/*\", step_metric=\"epoch/step\")\n",
        "\n",
        "        wandb.define_metric(\"train/step\")\n",
        "        wandb.define_metric(\"train/*\", step_metric=\"train/step\")\n",
        "\n",
        "        wandb.define_metric(\"validate/step\")\n",
        "        wandb.define_metric(\"validate/*\", step_metric=\"validate/step\")\n",
        "\n",
        "    # Loading from a starting point\n",
        "    if start_epoch > 0:\n",
        "        artifact_path = os.path.join(models_root_dir, f\"{run_name}_epoch_{start_epoch}.pth\")\n",
        "\n",
        "        checkpoint = torch.load(artifact_path, map_location=device)\n",
        "\n",
        "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
        "\n",
        "        train_step = checkpoint[\"train_step\"]+1\n",
        "        val_step = checkpoint[\"validate_step\"]+1\n",
        "\n",
        "    # Main Loop\n",
        "    for epoch in range(start_epoch+1, end_epoch+1):\n",
        "        print(\"-----------------------------\")\n",
        "        print(f\"Epoch {epoch}\")\n",
        "\n",
        "        lr = poly_lr_scheduler(optimizer, init_lr, epoch-1, max_iter=max_epoch)\n",
        "\n",
        "        print(f\"[Poly LR] 100xLR: {100.*lr:.6f}\")\n",
        "\n",
        "        train_loss, train_mIou, train_mIou_class, train_hist = train3(model, train_loader, criterion_train, optimizer)\n",
        "        train_loss, train_mIou, train_mIou_class, train_hist = train_4A(model, source_loader, target_loader, criterion_train, optimizer, beta, n, mean, std)\n",
        "        train_loss, train_mIou, train_mIou_class, train_hist = train_4B(model, source_loader, target_loader, criterion_train, optimizer, classmixer)\n",
        "        train_loss, train_mIou, train_mIou_class, train_hist = train5_1(model, source_loader, criterion_train, optimizer)\n",
        "        train_loss, train_mIou, train_mIou_class, train_hist = train5_2(model, source_loader, target_loader, criterion_train, optimizer, classmixer)\n",
        "\n",
        "        print(f'[Train Loss] : {train_loss:.6f} [mIoU]: {100.*train_mIou:.2f}%')\n",
        "        \n",
        "        val_loss, val_mIou, val_mIou_class, val_hist = validate(model, val_loader, criterion_val)\n",
        "\n",
        "        print(f'[Validation Loss] : {val_loss:.6f} [mIoU]: {100.*val_mIou:.2f}%')\n",
        "\n",
        "        if (epoch % 2) == 0:\n",
        "            checkpoint = {\n",
        "                \"model_state_dict\": model.state_dict(),\n",
        "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "                \"train_step\": train_step,\n",
        "                \"validate_step\": val_step,\n",
        "            }\n",
        "\n",
        "            file_name = f\"{run_name}_epoch_{epoch}.pth\"\n",
        "            \n",
        "            file_path = os.path.join(models_root_dir, file_name)\n",
        "            torch.save(checkpoint, file_path)\n",
        "\n",
        "            print(f\"Model saved to {file_path}\")\n",
        "\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            log_confusion_matrix(\"Confusion Matrix - Train\", train_hist.cpu().numpy(), \"epoch/train_confusion_matrix\", \"epoch/step\", epoch)\n",
        "            log_confusion_matrix(\"Confusion Matrix - Validate\", val_hist.cpu().numpy(), \"epoch/validate_confusion_matrix\", \"epoch/step\", epoch)\n",
        "\n",
        "            log_bar_chart_ioU(f\"Train IoU per class - epoch {epoch}\", [c.name for c in GTA5Labels_TaskCV2017().list_ if c.name != \"void\"], train_mIou, train_mIou_class.cpu().numpy(), \"epoch/train_Iou_class\", \"epoch/step\", epoch)\n",
        "            log_bar_chart_ioU(f\"Validate IoU per class - epoch {epoch}\", [c.name for c in GTA5Labels_TaskCV2017().list_ if c.name != \"void\"], val_mIou, val_mIou_class.cpu().numpy(), \"epoch/validate_Iou_class\", \"epoch/step\", epoch)\n",
        "\n",
        "    if end_epoch == max_epoch:\n",
        "        mean_latency, std_latency, mean_fps = latency(device, model, H=512, W=1024)\n",
        "\n",
        "        print(f\"[Num Flops]: {num_flops(device, model, 512, 1024)}\")\n",
        "        print(f\"[Mean Latency]: {mean_latency}\")\n",
        "        print(f\"[Std Latency]: {std_latency}\")\n",
        "        print(f\"[Mean FPS]: {mean_fps}\")\n",
        "        print(f\"[Num Param]: {num_param(model)}\")\n",
        "        \n",
        "pipeline()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 789
        },
        "id": "EHEnUiSXEpuJ",
        "outputId": "c00e4405-75ea-4d13-eedc-31ee112fc958"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhirUTIy-Kff"
      },
      "source": [
        "# Load another model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZ_U4Kbn8Nbd"
      },
      "outputs": [],
      "source": [
        "def load_model(artifact_name:str, file_prefix:str, device, id=None, epoch=50):\n",
        "    # model = BiSeNet(19, \"resnet18\").to(device)\n",
        "    # model = get_deeplab_v2(num_classes=n_classes, pretrain=True).to(device)\n",
        "    model = BiSeNetWithHRDA(n_classes, \"resnet18\", 0.5).to(device)\n",
        "\n",
        "    run = wandb.init(id=id, resume=\"allow\")\n",
        "\n",
        "    artifact = run.use_artifact(f'Machine_learning_and_Deep_learning_labs/Semantic Segmentation/{artifact_name}:epoch_{epoch}', type='model')\n",
        "    # artifact = run.use_artifact(f'Machine_learning_and_Deep_learning_labs/Semantic Segmentation/{artifact_name}:v72', type='model')\n",
        "    artifact_dir = artifact.download()\n",
        "    # artifact_dir = \"./artifacts/step_3B_BiSeNet-resnet18_GTA5-v42\"\n",
        "\n",
        "    artifact_path = os.path.join(artifact_dir, f\"{file_prefix}_epoch_{epoch}.pth\")\n",
        "\n",
        "    checkpoint = torch.load(artifact_path, map_location=device)\n",
        "\n",
        "    run.finish()\n",
        "\n",
        "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "Go_ulhhL9hkC",
        "outputId": "2451e17a-161a-4cc5-9eb0-8af68736b44e"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\"\n",
        "\n",
        "run_name = \"step_5_hrda_dacs_rcs\"\n",
        "backbone = \"BiSeNetHRDA\"\n",
        "context_path = \"resnet18\"\n",
        "dataset = \"Mixed\"\n",
        "epoch = 50\n",
        "model = load_model(artifact_name=f\"{run_name}_{backbone}-{context_path}_{dataset}\", file_prefix=run_name, device=device, id=\"zfe3iniu\", epoch=epoch)\n",
        "model.eval()\n",
        "None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 661
        },
        "id": "MnxBcOTxJB7H",
        "outputId": "e2d7c467-21ee-4804-d2af-fdb947f99cda"
      },
      "outputs": [],
      "source": [
        "B = 4\n",
        "H = 512\n",
        "W = 1024\n",
        "\n",
        "transform = v2.Compose([\n",
        "    v2.ToTensor(),\n",
        "    v2.Resize((H,W)),\n",
        "    # v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "target_transform = v2.Compose([\n",
        "    v2.ToTensor(),\n",
        "    v2.Resize((H, W), interpolation=v2.InterpolationMode.NEAREST),\n",
        "])\n",
        "\n",
        "# s = 0.5\n",
        "custom_augmentation = v2.Compose([\n",
        "    nn.Identity(),\n",
        "    # v2.GaussianBlur(7),\n",
        "    # v2.ColorJitter(brightness=s, contrast=s, saturation=s, hue=s),\n",
        "    # v2.GaussianNoise(mean=0, sigma=0.05)\n",
        "])\n",
        "\n",
        "# data = CityScapes(\"./Cityscapes/Cityspaces\", split=\"train\", transform=transform, target_transform=target_transform)\n",
        "data = CityScapes(\"./Cityscapes/Cityspaces\", split=\"val\", transform=img_transform_target, target_transform=label_transform_target)\n",
        "# data, _ = GTA5_dataset_splitter(\"./Gta5_extended\", train_split_percent=1.0, split_seed=42, augment=False, transform=transform, target_transform=target_transform)\n",
        "\n",
        "dataloader = DataLoader(data, batch_size=B, shuffle=True)\n",
        "# dataloader = DataLoader(data, batch_size=B, shuffle=False)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=255)\n",
        "\n",
        "ENABLE_WANDB_LOG = False\n",
        "# print(validate(model, dataloader, criterion))\n",
        "\n",
        "img_tensor, color_tensor, label = next(iter(dataloader))\n",
        "\n",
        "# img_tensor, label = randomcrop(img_tensor, label)\n",
        "\n",
        "img_tensor = \\\n",
        "    custom_augmentation(\n",
        "        img_tensor\n",
        "    # )\n",
        ").to(device)\n",
        "\n",
        "# img_tensor = img_tensor.clamp(0,1)\n",
        "\n",
        "# lr_img, lr_label, hr_img, hr_label, coords = model.hrda_crop(img_tensor, label.squeeze().to(device))\n",
        "\n",
        "# fused, (lr_out, hr_out) = model.hrda_forward(lr_img, hr_img, coords)\n",
        "\n",
        "# fused = model(normalize(img_tensor.detach()))\n",
        "fused, lr_out, hr_out, coords = model.hrda_eval(normalize(img_tensor).to(device))\n",
        "\n",
        "# fused, lr_out, hr_out, coords = model.hrda_eval(img_tensor)\n",
        "\n",
        "predicted_labels = fused.argmax(1).cpu()\n",
        "lr_predicted = lr_out.argmax(1).cpu()\n",
        "hr_predicted = hr_out.argmax(1).cpu()\n",
        "\n",
        "x1, y1, cw, ch = coords\n",
        "\n",
        "for i in range(B):\n",
        "    predicted_colors = decode_segmap(predicted_labels[i].numpy())\n",
        "    lr_colors = decode_segmap(lr_predicted[i].numpy())\n",
        "    hr_colors = decode_segmap(hr_predicted[i].numpy())\n",
        "    true_colors = decode_segmap(label[i, 0].numpy())\n",
        "    true_colors = decode_segmap(label[i, 0].detach().cpu().numpy())\n",
        "\n",
        "    fig, axes = plt.subplot_mosaic(\n",
        "        \"\"\"\n",
        "            AB\n",
        "            CD\n",
        "        \"\"\",\n",
        "        figsize=(7,7), layout=\"tight\"\n",
        "    )\n",
        "    # fig, ax = plt.subplots(2,2, figsize=(10,10), layout=\"tight\")\n",
        "\n",
        "    axes[\"A\"].set_title(\"Original segmentation map\")\n",
        "    axes[\"A\"].imshow(true_colors[y1:y1+ch, x1:x1+cw])\n",
        "    # axes[\"A\"].imshow(true_colors)\n",
        "    axes[\"A\"].axis('off')\n",
        "\n",
        "    axes[\"B\"].set_title(\"Predicted segmentation map\")\n",
        "    axes[\"B\"].imshow(predicted_colors[y1:y1+ch, x1:x1+cw])\n",
        "    # axes[\"B\"].imshow(predicted_colors)\n",
        "    axes[\"B\"].axis('off')\n",
        "\n",
        "    axes[\"C\"].set_title(\"Low resolution\")\n",
        "    axes[\"C\"].imshow(lr_colors[y1:y1+ch, x1:x1+cw])\n",
        "    # axes[\"C\"].imshow(lr_colors)\n",
        "    axes[\"C\"].axis('off')\n",
        "\n",
        "    axes[\"D\"].set_title(\"High resolution\")\n",
        "    axes[\"D\"].imshow(hr_colors)\n",
        "    axes[\"D\"].axis('off')\n",
        "\n",
        "    fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def log_confusion_matrix2(title:str, hist:np.ndarray):\n",
        "    row_sums = hist.sum(axis=1, keepdims=True)\n",
        "    safe_hist = np.where(row_sums == 0, 0, hist / row_sums)\n",
        "\n",
        "    fig = plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(100.*safe_hist, fmt=\".2f\", annot=True, cmap=\"Blues\", annot_kws={'size': 7})\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"True\")\n",
        "    plt.title(title)\n",
        "\n",
        "     # --- Diagonal Metrics ---\n",
        "    diag = np.diag(hist)\n",
        "    total = hist.sum()\n",
        "    overall_acc = diag.sum() / total if total > 0 else 0\n",
        "    per_class_acc = diag / row_sums.squeeze()\n",
        "    mean_class_acc = np.nanmean(per_class_acc)\n",
        "\n",
        "    # Optional: Frobenius norm-based diagonal dominance score\n",
        "    frob_total = np.linalg.norm(hist)\n",
        "    frob_offdiag = np.linalg.norm(hist - np.diagflat(diag))\n",
        "    diag_dominance_score = 1 - (frob_offdiag / frob_total) if frob_total > 0 else 0\n",
        "\n",
        "    print(f\"\\n📊 Overall Accuracy: {overall_acc:.4f}\")\n",
        "    print(f\"📈 Mean Per-Class Accuracy: {mean_class_acc:.4f}\")\n",
        "    print(f\"📐 Diagonal Dominance (Frobenius-based): {diag_dominance_score:.4f}\")\n",
        "\n",
        "def log_bar_chart_ioU2(title:str, class_names:list, mIou:float, iou_class:np.ndarray):\n",
        "    iou_percent = [round(iou*100., 2) for iou in iou_class]\n",
        "    miou_percent = round(mIou*100., 2)\n",
        "\n",
        "    all_labels = [\"mIoU\"] + class_names\n",
        "    all_values = [miou_percent] + iou_percent\n",
        "\n",
        "    fig = plt.figure(figsize=(14, 5))\n",
        "    bars = plt.bar(range(len(all_values)), all_values, color='skyblue')\n",
        "    plt.xticks(range(len(all_labels)), all_labels, rotation=45, ha=\"right\")\n",
        "\n",
        "    for i, bar in enumerate(bars):\n",
        "        height = bar.get_height()\n",
        "        plt.text(bar.get_x() + bar.get_width() / 2.0, height + 1, f'{height:.2f}',\n",
        "                ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "    plt.ylabel(\"IoU (%)\")\n",
        "    plt.ylim(0, 105)\n",
        "    plt.title(title)\n",
        "    plt.tight_layout()\n",
        "\n",
        "ENABLE_PRINT = False\n",
        "ENABLE_WANDB_LOG = False\n",
        "\n",
        "loss, val_mIou, val_mIou_class, val_hist = validate(model, dataloader, criterion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Loss: {loss}\\nmIou: {val_mIou}\\nmIou per classe: {val_mIou_class.tolist()}\")\n",
        "\n",
        "log_confusion_matrix2(f\"{run_name} - Confusion Matrix - Validate\", val_hist.cpu().numpy())\n",
        "\n",
        "log_bar_chart_ioU2(f\"{run_name} - Validate IoU per class - epoch {epoch}\", [c.name for c in GTA5Labels_TaskCV2017().list_ if c.name != \"void\"], val_mIou, val_mIou_class.cpu().numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# data = CityScapes(\"./Cityscapes/Cityspaces\", split=\"train\", transform=None, target_transform=None)\n",
        "# data = CityScapes(\"./Cityscapes/Cityspaces\", split=\"val\", transform=img_transform_target, target_transform=label_transform_target)\n",
        "data, _ = GTA5_dataset_splitter(\"./Gta5_extended\", train_split_percent=1.0, split_seed=42, augment=False, transform=None, target_transform=None)\n",
        "\n",
        "np.ndarray(next(iter(data))[0]).size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Class frequency calculation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data, _ = GTA5_dataset_splitter(\"./Gta5_extended\", train_split_percent=1.0, split_seed=42, augment=True, transform=v2.ToTensor(), target_transform=v2.ToTensor())\n",
        "dataloader = DataLoader(data, batch_size=1, shuffle=True)\n",
        "\n",
        "freqs = torch.zeros(n_classes)\n",
        "\n",
        "for _,_,label in tqdm(dataloader):\n",
        "    freqs += torch.bincount(label[label < n_classes].view(-1), minlength=n_classes)[:n_classes]\n",
        "\n",
        "    # plt.imshow(label[0].repeat(3,1,1).permute(1,2,0))\n",
        "\n",
        "print(freqs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "objects = GTA5Labels_TaskCV2017().list_\n",
        "\n",
        "print(\"id;name;count\")\n",
        "for i in range(n_classes):\n",
        "    print(f\"{objects[i].ID};{objects[i].name};{freqs[i].item()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "l = [2000279424,157799632,586026240,79183408,31111896,52304620,5818070,5331167,313206784,140294832,734290112,5793555,1213679,100922184,54996872,12020944,15980102,1665942,260148]\n",
        "\n",
        "a = torch.Tensor(l)\n",
        "\n",
        "a /= a.sum()\n",
        "\n",
        "fig, axs= plt.subplots(2,2)\n",
        "\n",
        "T = [[0.015, 0.02], [0.03, 0.06]]\n",
        "correction = torch.zeros(19)\n",
        "for i in range(2):\n",
        "    for j in range(2):\n",
        "        b = torch.exp((1-a)/T[i][j])/torch.sum(torch.exp((1-a)/T[i][j]))\n",
        "    \n",
        "        axs[i,j].plot(b)\n",
        "\n",
        "T = [[0.015, 0.02], [0.03, 0.06]]\n",
        "correction = torch.zeros(19)\n",
        "for i in range(2):\n",
        "    for j in range(2):\n",
        "        b = torch.exp((1-a)/T[i][j])/torch.sum(torch.exp((1-a)/T[i][j]))\n",
        "        \n",
        "        correction += 1-b/0.6\n",
        "\n",
        "        axs[i,j].plot((b+correction)/(b+correction).sum())\n",
        "        \n",
        "print(b.sum())\n",
        "plt.plot(b)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "Jntcuf0zNwaq",
        "c9TMCaEmg1N4",
        "JxswDJZOmLqQ",
        "BLfNdNzyMyxn",
        "gEx91uPqlFv3",
        "YZME_xiNNE-g",
        "LTki3m7aNH10",
        "u9m62PVUaXz6",
        "O2ybDTYzv0ZS",
        "XPTyvGv0fQd8",
        "_isSfs1sgJGE",
        "w0KuuGIWNAbF",
        "ON-XdXCkNCrn",
        "IhirUTIy-Kff",
        "3pafoSrDQ0es"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "166da3ac145f40d894f626ab60f82774": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "219f47ce4b8d4f3dac47e21d4d95c63f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26559a2ed8ae4244b098950164fa0eb5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d50ec68a6e6484dbef4b94d9b9bfd2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c62c4f0186b74b999cc2c7c8610f76de",
            "placeholder": "​",
            "style": "IPY_MODEL_cbc43d6d534e4d439f95a1d694f7ac00",
            "value": " 6216/6216 [00:44&lt;00:00, 39.77it/s]"
          }
        },
        "43d0c4a9221d45f9ba3a9c67eed8123d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4899af448b984d12bcc663899963447d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "581ce3cc835c4576a39255978c8526cb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "64cd3c2cc1d34100b8d861d14bcd6178": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6684ff67a8f14b94819fb4b3718be137": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "69c6f32570574572a57963c78fb9d864": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93c53ea0fe8c4fdd8a97ba1a5ee71af3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d0b6dc54747d46b5bf96f4b795b5627d",
              "IPY_MODEL_940ea8bba0c64f78961399d4855eeace",
              "IPY_MODEL_3d50ec68a6e6484dbef4b94d9b9bfd2a"
            ],
            "layout": "IPY_MODEL_26559a2ed8ae4244b098950164fa0eb5"
          }
        },
        "940ea8bba0c64f78961399d4855eeace": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e1d31ad13ffb4ad0a0a658c3f363eee6",
            "max": 6216,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_43d0c4a9221d45f9ba3a9c67eed8123d",
            "value": 6216
          }
        },
        "b6438ae4a5e4474bbbfa7dbb7c806a16": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_69c6f32570574572a57963c78fb9d864",
            "placeholder": "​",
            "style": "IPY_MODEL_6684ff67a8f14b94819fb4b3718be137",
            "value": " 7500/7500 [01:15&lt;00:00, 1693.13it/s]"
          }
        },
        "c62c4f0186b74b999cc2c7c8610f76de": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca53b671cecf4f09b535f69950e76158": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_581ce3cc835c4576a39255978c8526cb",
            "max": 7500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e59f8ab1c510475fb8c4d50a00a1df87",
            "value": 7500
          }
        },
        "cbc43d6d534e4d439f95a1d694f7ac00": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d0b6dc54747d46b5bf96f4b795b5627d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4899af448b984d12bcc663899963447d",
            "placeholder": "​",
            "style": "IPY_MODEL_166da3ac145f40d894f626ab60f82774",
            "value": "Extracting PNGs: 100%"
          }
        },
        "d143fab025d84a85abfe83c0767ebc28": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d39adf153c8445da9829b09973db1553": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d143fab025d84a85abfe83c0767ebc28",
            "placeholder": "​",
            "style": "IPY_MODEL_64cd3c2cc1d34100b8d861d14bcd6178",
            "value": "Extracting PNGs: 100%"
          }
        },
        "e1d31ad13ffb4ad0a0a658c3f363eee6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e59f8ab1c510475fb8c4d50a00a1df87": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f5f4c5d20eac43a7bee11061e5d36933": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d39adf153c8445da9829b09973db1553",
              "IPY_MODEL_ca53b671cecf4f09b535f69950e76158",
              "IPY_MODEL_b6438ae4a5e4474bbbfa7dbb7c806a16"
            ],
            "layout": "IPY_MODEL_219f47ce4b8d4f3dac47e21d4d95c63f"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
